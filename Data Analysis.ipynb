{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import statistics\n",
    "from sklearn.datasets import load_digits, load_iris, load_boston, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from graphviz import Digraph, Source, Graph\n",
    "from multiprocessing import cpu_count, Pool\n",
    "import sklearn\n",
    "from IPython.display import Math\n",
    "from sklearn.tree import export_graphviz\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adityavyas/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3020: DtypeWarning: Columns (17,55,59,61,65,68,69,70,83,90,91,92,93,120,121,122,123,126,140,141) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('ML1/Tab.delimited.Cleaned.dataset.WITH.variable.labels.csv', sep = '\\t', encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6344, 382)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>session_date</th>\n",
       "      <th>last_update_date</th>\n",
       "      <th>session_last_update_date</th>\n",
       "      <th>referrer</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>session_creation_date</th>\n",
       "      <th>expcomments</th>\n",
       "      <th>numparticipants_actual</th>\n",
       "      <th>numparticipants</th>\n",
       "      <th>...</th>\n",
       "      <th>diseaseforder</th>\n",
       "      <th>quoteorder</th>\n",
       "      <th>flagprimorder</th>\n",
       "      <th>sunkcostorder</th>\n",
       "      <th>anchorinorder</th>\n",
       "      <th>allowedforder</th>\n",
       "      <th>gamblerforder</th>\n",
       "      <th>moneypriorder</th>\n",
       "      <th>imaginedorder</th>\n",
       "      <th>iatorder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2400853</td>\n",
       "      <td>8/28/2013</td>\n",
       "      <td>8/28/13 12:15</td>\n",
       "      <td>8/28/13 12:15</td>\n",
       "      <td>abington</td>\n",
       "      <td>8/28/2013</td>\n",
       "      <td>8/28/2013</td>\n",
       "      <td>.</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2400856</td>\n",
       "      <td>8/28/2013</td>\n",
       "      <td>8/28/13 12:13</td>\n",
       "      <td>8/28/13 12:13</td>\n",
       "      <td>abington</td>\n",
       "      <td>8/28/2013</td>\n",
       "      <td>8/28/2013</td>\n",
       "      <td>.</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2400860</td>\n",
       "      <td>8/28/2013</td>\n",
       "      <td>8/28/13 12:15</td>\n",
       "      <td>8/28/13 12:15</td>\n",
       "      <td>abington</td>\n",
       "      <td>8/28/2013</td>\n",
       "      <td>8/28/2013</td>\n",
       "      <td>.</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2400868</td>\n",
       "      <td>8/28/2013</td>\n",
       "      <td>8/28/13 12:12</td>\n",
       "      <td>8/28/13 12:12</td>\n",
       "      <td>abington</td>\n",
       "      <td>8/28/2013</td>\n",
       "      <td>8/28/2013</td>\n",
       "      <td>.</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2400872</td>\n",
       "      <td>8/28/2013</td>\n",
       "      <td>8/28/13 12:11</td>\n",
       "      <td>8/28/13 12:11</td>\n",
       "      <td>abington</td>\n",
       "      <td>8/28/2013</td>\n",
       "      <td>8/28/2013</td>\n",
       "      <td>.</td>\n",
       "      <td></td>\n",
       "      <td>.</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 382 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   session_id session_date last_update_date session_last_update_date  \\\n",
       "0     2400853    8/28/2013    8/28/13 12:15            8/28/13 12:15   \n",
       "1     2400856    8/28/2013    8/28/13 12:13            8/28/13 12:13   \n",
       "2     2400860    8/28/2013    8/28/13 12:15            8/28/13 12:15   \n",
       "3     2400868    8/28/2013    8/28/13 12:12            8/28/13 12:12   \n",
       "4     2400872    8/28/2013    8/28/13 12:11            8/28/13 12:11   \n",
       "\n",
       "   referrer creation_date session_creation_date expcomments  \\\n",
       "0  abington     8/28/2013             8/28/2013           .   \n",
       "1  abington     8/28/2013             8/28/2013           .   \n",
       "2  abington     8/28/2013             8/28/2013           .   \n",
       "3  abington     8/28/2013             8/28/2013           .   \n",
       "4  abington     8/28/2013             8/28/2013           .   \n",
       "\n",
       "  numparticipants_actual numparticipants   ...    diseaseforder quoteorder  \\\n",
       "0                                      5   ...                4          3   \n",
       "1                                      5   ...                7          5   \n",
       "2                                      5   ...                2          8   \n",
       "3                                      5   ...                7          8   \n",
       "4                                      .   ...               10          3   \n",
       "\n",
       "  flagprimorder sunkcostorder anchorinorder allowedforder gamblerforder  \\\n",
       "0             7             2             8             6            10   \n",
       "1             2             6            11             9             1   \n",
       "2             9             3             5            10             6   \n",
       "3            10             6             9             4             1   \n",
       "4            11             6             9             5             4   \n",
       "\n",
       "  moneypriorder imaginedorder iatorder  \n",
       "0             1            11       12  \n",
       "1             4             8       12  \n",
       "2             4            11       12  \n",
       "3            11             2       12  \n",
       "4             1             2       12  \n",
       "\n",
       "[5 rows x 382 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Column Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mixed_columns = ['flagsupplement1', 'flagsupplement2', 'flagsupplement3', 'iatexplicitart1', 'iatexplicitart2',\n",
    "                'iatexplicitart3', 'iatexplicitart4', 'iatexplicitart5', 'iatexplicitart6', 'iatexplicitmath1',\n",
    "                'iatexplicitmath2', 'iatexplicitmath3', 'iatexplicitmath4', 'iatexplicitmath5', 'iatexplicitmath6',\n",
    "                'sysjust1', 'sysjust2', 'sysjust3', 'sysjust4', 'sysjust5', 'sysjust6', 'sysjust7', 'sysjust8',\n",
    "                'mturk.non.US', 'exprace']\n",
    "\n",
    "all_NAN_columns = ['task_status', 'task_sequence', 'beginlocaltime']\n",
    "\n",
    "numeric_columns = ['anchoring3ameter', 'anchoring3bmeter', 'anchoring1akm', 'anchoring1bkm', 'priorexposure8',\n",
    "                    'mturk.duplicate', 'mturk.exclude.null', 'mturk.keep', 'mturk.exclude', 'gamblerfallacya_sd',\n",
    "                    'gamblerfallacyb_sd', 'totexpmissed', 'numparticipants_actual', 'session_id', 'IATfilter',\n",
    "                    'priorexposure3', 'priorexposure4', 'priorexposure5', 'priorexposure6', 'priorexposure7', \n",
    "                    'priorexposure9', 'priorexposure10', 'priorexposure11', 'priorexposure12', 'priorexposure13',\n",
    "                    'numparticipants', 'age', 'anchoring1a', 'priorexposure1', 'priorexposure2', 'mturk.total.mini.exps',\n",
    "                    'anchoring1b', 'anchoring2a', 'anchoring2b', 'anchoring3a', 'anchoring3b', 'anchoring4a',\n",
    "                    'anchoring4b', 'artwarm', 'd_donotuse', 'ethnicity', 'flagdv1', 'flagdv2', 'flagdv3',\n",
    "                    'flagdv4', 'flagdv5', 'flagdv6', 'flagdv7', 'flagdv8', 'gamblerfallacya', 'gamblerfallacyb',\n",
    "                    'imaginedexplicit1', 'imaginedexplicit2', 'imaginedexplicit3', 'imaginedexplicit4',\n",
    "                    'major', 'mathwarm', 'moneyagea', 'moneyageb', 'moneygendera', 'moneygenderb', 'omdimc3rt',\n",
    "                    'omdimc3trt', 'quotea', 'quoteb', 'sunkcosta', 'sunkcostb', 'user_id', 'previous_session_id',\n",
    "                    'order', 'meanlatency', 'meanerror', 'block2_meanerror', 'block3_meanerror', 'block5_meanerror',\n",
    "                    'block6_meanerror', 'lat11', 'lat12', 'lat21', 'lat22', 'sd1', 'sd2', 'd_art1', 'd_art2', 'd_art',\n",
    "                    'sunkDV', 'anchoring1', 'anchoring2', 'anchoring3', 'anchoring4', 'Ranchori', 'RAN001',\n",
    "                    'RAN002', 'RAN003', 'Ranch1', 'Ranch2', 'Ranch3', 'Ranch4', 'gambfalDV', 'scalesreca', 'scalesrecb',\n",
    "                    'reciprocityother', 'quotearec', 'quotebrec', 'quote', 'totalflagestimations', 'totalnoflagtimeestimations',\n",
    "                    'flagdv', 'Sysjust', 'moneyfilter', 'Imagineddv', 'IATexpart', 'IATexpmath', 'IATexp.overall',\n",
    "                    'IATEXPfilter', 'scalesorder', 'reciprocorder', 'diseaseforder', 'quoteorder', 'flagprimorder',\n",
    "                    'sunkcostorder', 'anchorinorder', 'allowedforder', 'gamblerforder', 'moneypriorder', 'imaginedorder',\n",
    "                    'iatorder']\n",
    "\n",
    "date_columns = [col for col in data.columns if '_date' in col]\n",
    "\n",
    "non_string_columns = mixed_columns + all_NAN_columns + numeric_columns + date_columns\n",
    "string_columns = sorted([col for col in data.columns if col not in non_string_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert ['', ' ' and .] to NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data.copy()\n",
    "data2 = data2.replace({'': np.nan, ' ': np.nan, '.': np.nan})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Columns to Respective Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Numeric Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anchoring3ameter\n",
      "anchoring3bmeter\n",
      "anchoring1akm\n",
      "anchoring1bkm\n",
      "priorexposure8\n",
      "mturk.duplicate\n",
      "mturk.exclude.null\n",
      "mturk.keep\n",
      "mturk.exclude\n",
      "gamblerfallacya_sd\n",
      "gamblerfallacyb_sd\n",
      "totexpmissed\n",
      "numparticipants_actual\n",
      "session_id\n",
      "IATfilter\n",
      "priorexposure3\n",
      "priorexposure4\n",
      "priorexposure5\n",
      "priorexposure6\n",
      "priorexposure7\n",
      "priorexposure9\n",
      "priorexposure10\n",
      "priorexposure11\n",
      "priorexposure12\n",
      "priorexposure13\n",
      "numparticipants\n",
      "age\n",
      "anchoring1a\n",
      "priorexposure1\n",
      "priorexposure2\n",
      "mturk.total.mini.exps\n",
      "anchoring1b\n",
      "anchoring2a\n",
      "anchoring2b\n",
      "anchoring3a\n",
      "anchoring3b\n",
      "anchoring4a\n",
      "anchoring4b\n",
      "artwarm\n",
      "d_donotuse\n",
      "ethnicity\n",
      "flagdv1\n",
      "flagdv2\n",
      "flagdv3\n",
      "flagdv4\n",
      "flagdv5\n",
      "flagdv6\n",
      "flagdv7\n",
      "flagdv8\n",
      "gamblerfallacya\n",
      "gamblerfallacyb\n",
      "imaginedexplicit1\n",
      "imaginedexplicit2\n",
      "imaginedexplicit3\n",
      "imaginedexplicit4\n",
      "major\n",
      "mathwarm\n",
      "moneyagea\n",
      "moneyageb\n",
      "moneygendera\n",
      "moneygenderb\n",
      "omdimc3rt\n",
      "omdimc3trt\n",
      "quotea\n",
      "quoteb\n",
      "sunkcosta\n",
      "sunkcostb\n",
      "user_id\n",
      "previous_session_id\n",
      "order\n",
      "meanlatency\n",
      "meanerror\n",
      "block2_meanerror\n",
      "block3_meanerror\n",
      "block5_meanerror\n",
      "block6_meanerror\n",
      "lat11\n",
      "lat12\n",
      "lat21\n",
      "lat22\n",
      "sd1\n",
      "sd2\n",
      "d_art1\n",
      "d_art2\n",
      "d_art\n",
      "sunkDV\n",
      "anchoring1\n",
      "anchoring2\n",
      "anchoring3\n",
      "anchoring4\n",
      "Ranchori\n",
      "RAN001\n",
      "RAN002\n",
      "RAN003\n",
      "Ranch1\n",
      "Ranch2\n",
      "Ranch3\n",
      "Ranch4\n",
      "gambfalDV\n",
      "scalesreca\n",
      "scalesrecb\n",
      "reciprocityother\n",
      "quotearec\n",
      "quotebrec\n",
      "quote\n",
      "totalflagestimations\n",
      "totalnoflagtimeestimations\n",
      "flagdv\n",
      "Sysjust\n",
      "moneyfilter\n",
      "Imagineddv\n",
      "IATexpart\n",
      "IATexpmath\n",
      "IATexp.overall\n",
      "IATEXPfilter\n",
      "scalesorder\n",
      "reciprocorder\n",
      "diseaseforder\n",
      "quoteorder\n",
      "flagprimorder\n",
      "sunkcostorder\n",
      "anchorinorder\n",
      "allowedforder\n",
      "gamblerforder\n",
      "moneypriorder\n",
      "imaginedorder\n",
      "iatorder\n"
     ]
    }
   ],
   "source": [
    "for col in numeric_columns:\n",
    "    print(col)\n",
    "    data2[col] = pd.to_numeric(data2[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Date Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session_date\n",
      "last_update_date\n",
      "session_last_update_date\n",
      "creation_date\n",
      "session_creation_date\n",
      "task_creation_date.0\n",
      "task_creation_date.1\n",
      "task_creation_date.2\n",
      "task_creation_date.3\n",
      "task_creation_date.4\n",
      "task_creation_date.5\n",
      "task_creation_date.6\n",
      "task_creation_date.7\n",
      "task_creation_date.8\n",
      "task_creation_date.9\n",
      "task_creation_date.10\n",
      "task_creation_date.11\n",
      "task_creation_date.12\n",
      "task_creation_date.13\n",
      "task_creation_date.14\n",
      "task_creation_date.15\n",
      "task_creation_date.16\n",
      "task_creation_date.17\n",
      "task_creation_date.18\n",
      "task_creation_date.19\n",
      "task_creation_date.20\n",
      "task_creation_date.21\n",
      "task_creation_date.22\n",
      "task_creation_date.23\n",
      "task_creation_date.24\n",
      "task_creation_date.25\n",
      "task_creation_date.26\n",
      "task_creation_date.27\n",
      "task_creation_date.28\n",
      "task_creation_date.29\n",
      "task_creation_date.30\n",
      "task_creation_date.31\n",
      "task_creation_date.32\n",
      "task_creation_date.33\n",
      "task_creation_date.34\n",
      "task_creation_date.35\n",
      "task_creation_date.36\n",
      "task_creation_date.37\n",
      "task_creation_date.38\n",
      "task_creation_date.39\n",
      "task_creation_date.40\n",
      "task_creation_date.41\n",
      "task_creation_date.42\n",
      "task_creation_date.43\n",
      "task_creation_date.44\n",
      "task_creation_date.45\n"
     ]
    }
   ],
   "source": [
    "for col in date_columns:\n",
    "    print(col)\n",
    "    data2[col] = pd.to_datetime(data2[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. String Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ContactGroup\n",
      "MoneyGroup\n",
      "allowedforbidden\n",
      "allowedforbiddenGroup\n",
      "allowedforbiddena\n",
      "allowedforbiddenb\n",
      "anch1group\n",
      "anch2group\n",
      "anch3group\n",
      "anch4group\n",
      "citizenship\n",
      "citizenship2\n",
      "compensation\n",
      "diseaseframinga\n",
      "diseaseframingb\n",
      "expcomments\n",
      "expgender\n",
      "exprunafter\n",
      "exprunafter2\n",
      "feedback\n",
      "filter_$\n",
      "flagGroup\n",
      "flagfilter\n",
      "flagtimeestimate1\n",
      "flagtimeestimate2\n",
      "flagtimeestimate3\n",
      "flagtimeestimate4\n",
      "gainlossDV\n",
      "gainlossgroup\n",
      "gambfalgroup\n",
      "iat_exclude\n",
      "imagineddescribe\n",
      "imptaskto\n",
      "lab_or_online\n",
      "moneyethnicitya\n",
      "moneyethnicityb\n",
      "mturk.Submitted.PaymentReq\n",
      "nativelang\n",
      "nativelang2\n",
      "noflagtimeestimate1\n",
      "noflagtimeestimate2\n",
      "noflagtimeestimate3\n",
      "noflagtimeestimate4\n",
      "o1\n",
      "o10\n",
      "o11\n",
      "o2\n",
      "o3\n",
      "o4\n",
      "o5\n",
      "o6\n",
      "o7\n",
      "o8\n",
      "o9\n",
      "omdimc3\n",
      "partgender\n",
      "politicalid\n",
      "previous_session_schema\n",
      "quoteGroup\n",
      "race\n",
      "reciprocitygroup\n",
      "reciprocityothera\n",
      "reciprocityotherb\n",
      "reciprocityus\n",
      "reciprocityusa\n",
      "reciprocityusb\n",
      "recruitment\n",
      "referrer\n",
      "religion\n",
      "sample\n",
      "scales\n",
      "scalesa\n",
      "scalesb\n",
      "scalesgroup\n",
      "separatedornot\n",
      "session_created_by\n",
      "session_status\n",
      "sex\n",
      "study_name\n",
      "study_url\n",
      "sunkgroup\n",
      "task_id.0\n",
      "task_id.1\n",
      "task_id.10\n",
      "task_id.11\n",
      "task_id.12\n",
      "task_id.13\n",
      "task_id.14\n",
      "task_id.15\n",
      "task_id.16\n",
      "task_id.17\n",
      "task_id.18\n",
      "task_id.19\n",
      "task_id.2\n",
      "task_id.20\n",
      "task_id.21\n",
      "task_id.22\n",
      "task_id.23\n",
      "task_id.24\n",
      "task_id.25\n",
      "task_id.26\n",
      "task_id.27\n",
      "task_id.28\n",
      "task_id.29\n",
      "task_id.3\n",
      "task_id.30\n",
      "task_id.31\n",
      "task_id.32\n",
      "task_id.33\n",
      "task_id.34\n",
      "task_id.35\n",
      "task_id.36\n",
      "task_id.37\n",
      "task_id.38\n",
      "task_id.39\n",
      "task_id.4\n",
      "task_id.40\n",
      "task_id.41\n",
      "task_id.42\n",
      "task_id.43\n",
      "task_id.44\n",
      "task_id.45\n",
      "task_id.5\n",
      "task_id.6\n",
      "task_id.7\n",
      "task_id.8\n",
      "task_id.9\n",
      "task_url.0\n",
      "task_url.1\n",
      "task_url.10\n",
      "task_url.11\n",
      "task_url.12\n",
      "task_url.13\n",
      "task_url.14\n",
      "task_url.15\n",
      "task_url.16\n",
      "task_url.17\n",
      "task_url.18\n",
      "task_url.19\n",
      "task_url.2\n",
      "task_url.20\n",
      "task_url.21\n",
      "task_url.22\n",
      "task_url.23\n",
      "task_url.24\n",
      "task_url.25\n",
      "task_url.26\n",
      "task_url.27\n",
      "task_url.28\n",
      "task_url.29\n",
      "task_url.3\n",
      "task_url.30\n",
      "task_url.31\n",
      "task_url.32\n",
      "task_url.33\n",
      "task_url.34\n",
      "task_url.35\n",
      "task_url.36\n",
      "task_url.37\n",
      "task_url.38\n",
      "task_url.39\n",
      "task_url.4\n",
      "task_url.40\n",
      "task_url.41\n",
      "task_url.42\n",
      "task_url.43\n",
      "task_url.44\n",
      "task_url.45\n",
      "task_url.5\n",
      "task_url.6\n",
      "task_url.7\n",
      "task_url.8\n",
      "task_url.9\n",
      "text\n",
      "us_or_international\n",
      "user_agent\n"
     ]
    }
   ],
   "source": [
    "for col in string_columns:\n",
    "    print(col)\n",
    "    data2[col] = data2[col].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Synthetic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder():\n",
    "    \n",
    "    SigmoidActivation = \"sigmoid\"\n",
    "    ReLUActivation = \"relu\"\n",
    "    LinearActivation = \"linear\"\n",
    "    LeakyReLUActivation = \"lrelu\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 learning_rate = 0.04, \n",
    "                 batch_size = 32,\n",
    "                 num_hidden_layers = None,\n",
    "                 num_neurons_each_layer = None,\n",
    "                 z_shape = 20,\n",
    "                 epochs = 10):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_neurons_each_layer = num_neurons_each_layer\n",
    "        self.z_shape = z_shape\n",
    "        \n",
    "        self.activations_functions = {\n",
    "            self.SigmoidActivation: self._sigmoid,\n",
    "            self.LeakyReLUActivation: self._leaky_relu,\n",
    "            self.ReLUActivation: self._relu,\n",
    "            self.LinearActivation: self._linear\n",
    "        }\n",
    "        self.activations_derivatives = {\n",
    "            self.SigmoidActivation: self._sigmoid_derivative,\n",
    "            self.LeakyReLUActivation: self._leaky_relu_derivative,\n",
    "            self.ReLUActivation: self._relu_derivative,\n",
    "            self.LinearActivation: self._linear_derivative\n",
    "        }\n",
    "        \n",
    "        # Activations for Encoder and Decoder\n",
    "        self.encoder_activations = [self.LeakyReLUActivation] * self.num_hidden_layers + [self.LinearActivation]\n",
    "        self.decoder_activations = [self.ReLUActivation] * self.num_hidden_layers + [self.SigmoidActivation]\n",
    "        \n",
    "        self.num_neurons_each_encoder_layer = self.num_neurons_each_layer\n",
    "        self.num_neurons_each_decoder_layer = self.num_neurons_each_layer[::-1]\n",
    "        \n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        x = np.select([x < 0, x >= 0], [1 - 1/(1 + np.exp(x)), 1/(1 + np.exp(-x))])\n",
    "        return x\n",
    "\n",
    "    def _relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def _leaky_relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def _linear(self, x):\n",
    "        return x\n",
    "\n",
    "    def _sigmoid_derivative(self, x):\n",
    "        return self._sigmoid(x) * (1 - self._sigmoid(x))\n",
    "\n",
    "    def _relu_derivative(self, x):\n",
    "        return (np.ones_like(x) * (x > 0))\n",
    "    \n",
    "    def _leaky_relu_derivative(self, x):\n",
    "        return\n",
    "\n",
    "    def _linear_derivative(self, x):\n",
    "        return np.ones_like(x)\n",
    "    \n",
    "    def _binary_cross_entropy_loss(self, y_hat, y):\n",
    "        loss = np.sum(-y * np.log(y_hat + 1e-15) - (1 - y) * np.log(1 - y_hat + 1e-15))\n",
    "        return loss\n",
    "    \n",
    "    def _encoder(self, X):\n",
    "        encoder_out = []\n",
    "        \n",
    "        for curr_layer in self.encoder_layers:\n",
    "            encoder_out.append([])\n",
    "                \n",
    "            # Get the activation for this layer and its function\n",
    "            activation_for_this_layer = self.encoder_activations[curr_layer]\n",
    "            activation_function = self.activations_functions[activation_for_this_layer]\n",
    "\n",
    "            if curr_layer == 0:\n",
    "                previous_layer_output = X\n",
    "            else:\n",
    "                previous_layer_output = encoder_out[curr_layer - 1].copy()\n",
    "                previous_layer_output = np.insert(previous_layer_output, obj = 0, values = 1, axis = 1)\n",
    "            \n",
    "            if curr_layer != self.encoder_layers[-1]:\n",
    "                encoder_out[curr_layer] = activation_function(previous_layer_output @ self.encoder_weights[curr_layer].T)\n",
    "            else:\n",
    "                encoder_weights_last_layer = np.transpose(self.encoder_weights[curr_layer], axes = (0, 2, 1))\n",
    "                encoder_out[curr_layer] = activation_function(previous_layer_output @ encoder_weights_last_layer)\n",
    "        \n",
    "        encoder_out = np.array(encoder_out)\n",
    "        mu, log_sigma = encoder_out[-1][0], encoder_out[-1][1]\n",
    "        return mu, log_sigma\n",
    "    \n",
    "    def _decoder(self, z):\n",
    "        \n",
    "        decoder_out = []\n",
    "        \n",
    "        for curr_layer in self.decoder_layers:\n",
    "            decoder_out.append([])\n",
    "                \n",
    "            # Get the activation for this layer and its function\n",
    "            activation_for_this_layer = self.decoder_activations[curr_layer]\n",
    "            activation_function = self.activations_functions[activation_for_this_layer]\n",
    "\n",
    "            if curr_layer == 0:\n",
    "                previous_layer_output = z\n",
    "            else:\n",
    "                previous_layer_output = decoder_out[curr_layer - 1].copy()\n",
    "            previous_layer_output = np.insert(previous_layer_output, obj = 0, values = 1, axis = 1)\n",
    "            \n",
    "            decoder_out[curr_layer] = activation_function(previous_layer_output @ self.decoder_weights[curr_layer].T)\n",
    "        \n",
    "        decoder_out = decoder_out[-1]\n",
    "        return decoder_out\n",
    "    \n",
    "    def _forward(self, X):\n",
    "        \n",
    "        # Encode\n",
    "        mu, log_sigma = self._encoder(X)\n",
    "        \n",
    "        # Reparametrization trick to sample z from gaussian. First sample from standar normal distribution.\n",
    "        # Then we use z = mu + sigma*x to get our latent variable.\n",
    "        self.rand_sample = np.random.standard_normal(size = (self.batch_size, self.z_shape))\n",
    "        self.sample_z = mu + np.exp(log_sigma * .5) * self.rand_sample\n",
    "        \n",
    "        # Decode\n",
    "        decoder_out = self._decoder(self.sample_z)\n",
    "        \n",
    "        return decoder_out, mu, log_sigma\n",
    "    \n",
    "    def _backward(self):\n",
    "        return\n",
    "    \n",
    "    def _initialise_weights(self, input_shape):\n",
    "        \n",
    "        # Encoder Layers\n",
    "        self.num_neurons_each_encoder_layer.append(2) # 2 for two outputs - mu and sigma\n",
    "        self.total_encoder_layers = self.num_hidden_layers + 1 # +1 for the last output layer\n",
    "        self.encoder_layers = range(self.total_encoder_layers)\n",
    "        \n",
    "        # Decoder Layers\n",
    "        self.num_neurons_each_decoder_layer.append(input_shape) # Last layer of decoder has input shape\n",
    "        self.total_decoder_layers = self.num_hidden_layers + 1 # +1 for the last output layer\n",
    "        self.decoder_layers = range(self.total_decoder_layers)\n",
    "\n",
    "        # Empty weight arrays\n",
    "        self.encoder_weights = []\n",
    "        self.decoder_weights = []\n",
    "\n",
    "        # Initialise encoder weights\n",
    "        for layer in self.encoder_layers:\n",
    "            self.encoder_weights.append([])\n",
    "\n",
    "            number_of_neurons_in_this_layer = self.num_neurons_each_encoder_layer[layer]\n",
    "            if layer == 0:\n",
    "                fan_in = input_shape\n",
    "                fan_out = number_of_neurons_in_this_layer\n",
    "                previous_layer_shape = fan_in\n",
    "            else:\n",
    "                fan_in = self.num_neurons_each_encoder_layer[layer - 1]\n",
    "                fan_out = number_of_neurons_in_this_layer\n",
    "                previous_layer_shape =  1 + fan_in\n",
    "            \n",
    "            if layer != self.encoder_layers[-1]:\n",
    "                self.encoder_weights[layer] = np.random.uniform(low = -2.0/(fan_in + fan_out),\n",
    "                                                                high = 2.0/(fan_in + fan_out),\n",
    "                                                                size = (number_of_neurons_in_this_layer,\n",
    "                                                                       previous_layer_shape))\n",
    "            else:\n",
    "                # Last layer of encoder outputs mu and sigma whose dimensions\n",
    "                # are of shape z_shape.\n",
    "                self.encoder_weights[layer] = np.random.uniform(low = -2.0/(fan_in + fan_out),\n",
    "                                                                high = 2.0/(fan_in + fan_out),\n",
    "                                                                size = (number_of_neurons_in_this_layer,\n",
    "                                                                       self.z_shape,\n",
    "                                                                       previous_layer_shape))\n",
    "                \n",
    "                \n",
    "                \n",
    "        # Initialise decoder weights\n",
    "        for layer in self.decoder_layers:\n",
    "            self.decoder_weights.append([])\n",
    "\n",
    "            number_of_neurons_in_this_layer = self.num_neurons_each_decoder_layer[layer]\n",
    "            if layer == 0:\n",
    "                # Input to decoder is the latent variable constructed from\n",
    "                # gaussian distribution\n",
    "                fan_in = self.z_shape\n",
    "            else:\n",
    "                fan_in = self.num_neurons_each_layer[layer - 1]\n",
    "            \n",
    "            fan_out = number_of_neurons_in_this_layer\n",
    "            previous_layer_shape = 1 + fan_in\n",
    "            self.decoder_weights[layer] = np.random.uniform(low = -2.0/(fan_in + fan_out),\n",
    "                                                            high = 2.0/(fan_in + fan_out),\n",
    "                                                            size = (number_of_neurons_in_this_layer, \n",
    "                                                                   previous_layer_shape))\n",
    "\n",
    "        self.encoder_weights = np.array(self.encoder_weights)\n",
    "        self.decoder_weights = np.array(self.decoder_weights)\n",
    "        self.old_encoder_weights = deepcopy(self.encoder_weights)\n",
    "        self.old_decoder_weights = deepcopy(self.decoder_weights)\n",
    "        \n",
    "    def _get_batches(self, X):\n",
    "        for i in range(0, X.shape[0], self.batch_size):\n",
    "            yield X[i: i + self.batch_size]\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \n",
    "        # Add a bias column to X\n",
    "        X_new = np.column_stack((np.ones(len(X)), X))\n",
    "        \n",
    "        # Initialise weights using Glorot Uniform initialiser\n",
    "        self._initialise_weights(X_new.shape[1])\n",
    "        \n",
    "        # Get batches\n",
    "        batches = self._get_batches(X_new)\n",
    "        \n",
    "        iterations = 0\n",
    "        while iterations <= self.epochs:\n",
    "            \n",
    "            # Train using mini-batch SGD\n",
    "            for x_batch in batches:\n",
    "                \n",
    "                # Forward pass\n",
    "                out, mu, log_sigma = self._forward(x_batch)\n",
    "                \n",
    "                # Reconstruction Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.68266696 0.5713713  0.12659436 0.81292579 0.48890174 0.18363954\n",
      "  0.57537288 0.49415602 0.40522787 0.47704483 0.23208769 0.43455381\n",
      "  0.46747268 0.6954429  0.70064705 0.52701089 0.79557236 0.93616276\n",
      "  0.80445233 0.10518495 0.64954404 0.17540005 0.17630946 0.3074999\n",
      "  0.07983031 0.47142763 0.10381405 0.58969382 0.49564933 0.10087955\n",
      "  0.30884341]\n",
      " [0.49961903 0.49099358 0.4969924  0.49646962 0.49949895 0.48845431\n",
      "  0.49969168 0.4857558  0.50982183 0.50457645 0.49103865 0.49466304\n",
      "  0.49511137 0.5060591  0.50848698 0.50872141 0.49063465 0.50457978\n",
      "  0.49776493 0.50393182 0.49222342 0.4980785  0.49553691 0.48611342\n",
      "  0.48675214 0.51377622 0.48889061 0.48897981 0.51020847 0.49058444\n",
      "  0.48986356]\n",
      " [0.50465918 0.46782867 0.51486879 0.50802514 0.4943199  0.47005183\n",
      "  0.51466763 0.49162345 0.5099383  0.4910997  0.48497425 0.49784574\n",
      "  0.49781357 0.50465652 0.49005116 0.5176684  0.49520983 0.51290347\n",
      "  0.48525044 0.51752813 0.49194769 0.49831259 0.48038102 0.47093954\n",
      "  0.48180639 0.52201861 0.49429928 0.48642499 0.48510874 0.49985616\n",
      "  0.49780297]\n",
      " [0.49836906 0.48592273 0.49842227 0.50242839 0.49325393 0.48274692\n",
      "  0.50248615 0.48416852 0.51012685 0.49842434 0.49189372 0.48972932\n",
      "  0.49601526 0.5123738  0.5061561  0.50961535 0.49742429 0.51342139\n",
      "  0.49687512 0.50528621 0.49600178 0.49551058 0.48952259 0.48042833\n",
      "  0.48593756 0.51459773 0.49034    0.48374688 0.50354013 0.48614261\n",
      "  0.48929188]\n",
      " [0.50305616 0.48974127 0.49393501 0.50073755 0.50052039 0.48309271\n",
      "  0.50213448 0.48732058 0.50852478 0.50401271 0.48642825 0.49606165\n",
      "  0.49494672 0.50612384 0.50856197 0.5101142  0.49324197 0.51080834\n",
      "  0.50009927 0.49951349 0.49284057 0.49449736 0.49057991 0.48326045\n",
      "  0.47931633 0.51437746 0.48323457 0.49140307 0.50805451 0.48732746\n",
      "  0.48914296]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 's' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-315-57deb3211c37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariationalAutoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_neurons_each_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-314-5c929c8de536>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_sigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                 \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 's' is not defined"
     ]
    }
   ],
   "source": [
    "vae = VariationalAutoencoder(num_hidden_layers = 1, num_neurons_each_layer = [3], batch_size = 5)\n",
    "vae.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression():\n",
    "    \n",
    "    def __init__(self, bias = None, weights = None, fit_intercept = True):\n",
    "        self.bias = bias\n",
    "        self.weights = weights\n",
    "        self.fit_intercept = fit_intercept\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if self.fit_intercept:\n",
    "            X = np.column_stack((np.ones(len(X)), X))\n",
    "        else:\n",
    "            X = np.column_stack((np.zeros(len(X)), X))\n",
    "        self.all_weights = np.linalg.inv(np.dot(X.T, X)).dot(X.T).dot(y)\n",
    "        self.weights = self.all_weights[1:]\n",
    "        self.bias = self.all_weights[0]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        self.weights = self.weights.reshape(1, -1)\n",
    "        predictions = self.bias + np.dot(self.weights, X.T)\n",
    "        return predictions[0]\n",
    "    \n",
    "    def get_mse(self, y_true, y_pred):\n",
    "        return np.sum((y_true - y_pred)**2)/y_true.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeRegression():\n",
    "    \n",
    "    def __init__(self, \n",
    "                 bias = None, \n",
    "                 weights = None, \n",
    "                 lambda_param = 10, \n",
    "                 fit_intercept = True):\n",
    "        self.bias = bias\n",
    "        self.weights = weights\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.lambda_param = lambda_param\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if self.fit_intercept:\n",
    "            X = np.column_stack((np.ones(len(X)), X))\n",
    "        else:\n",
    "            X = np.column_stack((np.zeros(len(X)), X))\n",
    "            \n",
    "        self.all_weights = np.linalg.inv(np.dot(X.T, X) + \\\n",
    "                            self.lambda_param * np.identity(X.shape[1])).dot(X.T).dot(y)\n",
    "        self.weights = self.all_weights[1:]\n",
    "        self.bias = self.all_weights[0]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        self.weights = self.weights.reshape(1, -1)\n",
    "        predictions = self.bias + np.dot(self.weights, X.T)\n",
    "        return predictions[0]\n",
    "    \n",
    "    def get_mse(self, y_true, y_pred):\n",
    "        return np.sum((y_true - y_pred)**2)/y_true.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LassoRegression():\n",
    "    \n",
    "    def __init__(self, \n",
    "                 bias = None, \n",
    "                 weights = None, \n",
    "                 lambda_param = 10, \n",
    "                 max_iters = 100, \n",
    "                 fit_intercept = True):\n",
    "        self.bias = 0    \n",
    "        self.lambda_param = lambda_param\n",
    "        self.max_iters = max_iters\n",
    "        self.fit_intercept = fit_intercept\n",
    "    \n",
    "    def _soft_threshold(self, x, lambda_):\n",
    "        if x > 0.0 and lambda_ < abs(x):\n",
    "            return x - lambda_\n",
    "        elif x < 0.0 and lambda_ < abs(x):\n",
    "            return x + lambda_\n",
    "        else:\n",
    "            return 0.0\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        if self.fit_intercept:\n",
    "            X = np.column_stack((np.ones(len(X)), X))\n",
    "        else:\n",
    "            X = np.column_stack((np.zeros(len(X)), X))\n",
    "            \n",
    "        row_length, column_length = X.shape\n",
    "        \n",
    "        # Define the weights\n",
    "        self.weights = np.zeros((1, column_length))[0]\n",
    "        if self.fit_intercept:\n",
    "            self.weights[0] = np.sum(y - \\\n",
    "                                np.dot(X[:, 1:], self.weights[1:]))/(X.shape[0])\n",
    "        \n",
    "        #Looping until max number of iterations\n",
    "        for iteration in range(self.max_iters):\n",
    "            start = 1 if self.fit_intercept else 0\n",
    "            \n",
    "            #Looping through each coordinate\n",
    "            for j in range(start, column_length):\n",
    "                    \n",
    "                tmp_weights = self.weights.copy()\n",
    "                tmp_weights[j] = 0.0\n",
    "                r_j = y - np.dot(X, tmp_weights)\n",
    "                arg1 = np.dot(X[:, j], r_j)\n",
    "                arg2 = self.lambda_param * X.shape[0]\n",
    "\n",
    "                self.weights[j] = self._soft_threshold(arg1, arg2)/(X[:, j]**2).sum()\n",
    "\n",
    "                if self.fit_intercept:\n",
    "                    self.weights[0] = np.sum(y - \\\n",
    "                                        np.dot(X[:, 1:], self.weights[1:]))/(X.shape[0])\n",
    "        \n",
    "        self.bias = self.weights[0]\n",
    "        self.weights = self.weights[1:]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        self.weights = self.weights.reshape(1, -1)\n",
    "        predictions = self.bias + np.dot(self.weights, X.T)\n",
    "        return predictions[0]\n",
    "    \n",
    "    def get_mse(self, y_true, y_pred):\n",
    "        return np.sum((y_true - y_pred)**2)/y_true.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "    \n",
    "    def __init__(self,  \n",
    "                 weights = None, \n",
    "                 bias = None, \n",
    "                 fit_intercept = True,\n",
    "                 decision_threshold = 0.5,\n",
    "                 epochs = 50,\n",
    "                 solver = 'sgd',\n",
    "                 batch_size = 30,\n",
    "                 learning_rate = 0.05,\n",
    "                 tolerance = 1e-13):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.tolerance = tolerance\n",
    "        self.decision_threshold = decision_threshold\n",
    "        self.epochs = epochs\n",
    "        self.solver = solver\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.solver_func = {'newton': self._newton_solver,\n",
    "                            'sgd': self._sgd_solver}\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "        \n",
    "    def _log_likelihood(self, X, y):\n",
    "        P = self._sigmoid(X @ self.weights)\n",
    "        P = P.reshape(-1, 1)\n",
    "        log_P = np.log(P + 1e-16)\n",
    "\n",
    "        P_ = 1 - P\n",
    "        log_P_ = np.log(P_ + 1e-16)\n",
    "        log_likelihood = np.sum(y*log_P + (1 - y)*log_P_)\n",
    "        return log_likelihood\n",
    "    \n",
    "    def _get_true_class_labels(self, labels):\n",
    "        true_labels = np.array([self.class_range_to_actual_classes[i] for i in labels])\n",
    "        return true_labels\n",
    "    \n",
    "    def _get_batches(self, X, y):\n",
    "        for i in range(0, X.shape[0], self.batch_size):\n",
    "            yield (X[i: i + self.batch_size], y[i: i + self.batch_size])\n",
    "            \n",
    "    def _convert_y(self, y):\n",
    "        self.actual_classes = sorted(np.unique(y))\n",
    "        self.class_range = [0, 1]\n",
    "        self.class_range_to_actual_classes = dict(zip(*(self.class_range, self.actual_classes)))\n",
    "        self.actual_classes_to_class_range = dict(zip(*(self.actual_classes, self.class_range)))\n",
    "        \n",
    "        y_ = np.array([self.actual_classes_to_class_range[i] for i in y])\n",
    "        y_ = y_.reshape(-1, 1)\n",
    "        return y_\n",
    "    \n",
    "    def _newton_solver(self, X, y):\n",
    "        log_likelihood = self._log_likelihood(X, y)\n",
    "        iterations = 0\n",
    "        delta = np.inf\n",
    "        while(np.abs(delta) > self.tolerance and iterations < self.epochs):\n",
    "            iterations += 1\n",
    "            \n",
    "            # Calculate positive class probabilities: p = sigmoid(W*x + b)\n",
    "            z = X @ self.weights\n",
    "            P = self._sigmoid(z)\n",
    "            P = P.reshape(-1, 1)\n",
    "\n",
    "            # First derivative of loss w.r.t weights\n",
    "            grad = X.T @ (P - y)\n",
    "\n",
    "            # Hessian of loss w.r.t weights\n",
    "            P_ = 1 - P\n",
    "            W = P * P_\n",
    "            W = W.reshape(1, -1)[0]\n",
    "            W = np.diag(W)\n",
    "            hess = X.T @ W @ X\n",
    "\n",
    "            # Weight update using Newton-Rhapson Method\n",
    "            self.weights -= np.linalg.inv(hess) @ grad\n",
    "            \n",
    "            # Calculate new log likelihood\n",
    "            new_log_likelihood = self._log_likelihood(X, y)\n",
    "            delta = log_likelihood - new_log_likelihood\n",
    "            log_likelihood = new_log_likelihood\n",
    "    \n",
    "    def _sgd_solver(self, X, y):\n",
    "        iterations = 0\n",
    "        while(iterations < self.epochs):\n",
    "            iterations += 1\n",
    "            \n",
    "            # Get batches\n",
    "            batches = self._get_batches(X, y)\n",
    "            \n",
    "            # Update weights using Mini batch stochastic gradient descent\n",
    "            for (x_batch, y_batch) in batches:\n",
    "                \n",
    "                # Raw output\n",
    "                z = x_batch @ self.weights\n",
    "                \n",
    "                # Calculate positive class probabilities: p = sigmoid(W*x + b)\n",
    "                P = self._sigmoid(z)\n",
    "\n",
    "                # First derivative of loss w.r.t weights\n",
    "                grad = x_batch.T @ (P - y_batch)\n",
    "                \n",
    "                # Update weights\n",
    "                self.weights -= self.learning_rate * grad\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if self.fit_intercept:\n",
    "            X = np.column_stack((np.ones(len(X)), X))\n",
    "        else:\n",
    "            X = np.column_stack((np.zeros(len(X)), X))\n",
    "        row_length, column_length = X.shape\n",
    "        \n",
    "        # Define the weights\n",
    "        self.weights = np.zeros((column_length, 1))\n",
    "        \n",
    "        # Convert y to {0, 1}\n",
    "        y = self._convert_y(y)\n",
    "        \n",
    "        # Use the solver\n",
    "        self.solver_func[self.solver](X, y)\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        \n",
    "        if self.fit_intercept:\n",
    "            X = np.column_stack((np.ones(len(X)), X))\n",
    "        else:\n",
    "            X = np.column_stack((np.zeros(len(X)), X))\n",
    "        \n",
    "        z = X @ self.weights\n",
    "        predicted_probs = self._sigmoid(z)\n",
    "        return predicted_probs\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predict_probs = self.predict_proba(X)\n",
    "        preds = np.where(predict_probs < 0.5, 0, 1).flatten()\n",
    "        true_preds = self._get_true_class_labels(preds)\n",
    "        return true_preds\n",
    "    \n",
    "    def get_accuracy(self, y, y_hat):\n",
    "        return sum(y == y_hat)*100/len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassLogisticRegression():\n",
    "    \n",
    "    def __init__(self,  \n",
    "                 weights = None, \n",
    "                 bias = None, \n",
    "                 fit_intercept = True,\n",
    "                 epochs = 50,\n",
    "                 learning_rate = 0.05,\n",
    "                 batch_size = 50):\n",
    "        self.weights = weights\n",
    "        self.learning_rate = learning_rate\n",
    "        self.bias = bias\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def _softmax(self, z):\n",
    "        \n",
    "        # We only calculate the softmax probabilities of the first (K-1) classes\n",
    "        z_ = z[:, :(z.shape[1] - 1)]\n",
    "        e_x = np.exp(z_)\n",
    "        out_k_minus_1 = e_x / (1 + e_x.sum(axis = 1, keepdims = True))\n",
    "        \n",
    "        # Probability for last K = 1 - p((K - 1))\n",
    "        out_k = 1 - out_k_minus_1.sum(axis = 1)\n",
    "        out = np.column_stack((out_k_minus_1, out_k))\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def _get_true_class_labels(self, P):\n",
    "        labels = P.argmax(axis = 1)\n",
    "        labels = np.array([self.class_range_to_actual_classes[i] for i in labels])\n",
    "        return labels\n",
    "    \n",
    "    def _calculate_cross_entropy(self, y, log_yhat):\n",
    "        return -np.sum(y * log_yhat, axis = 1)\n",
    "    \n",
    "    def _convert_to_indicator(self, y):\n",
    "        y_indicator = np.zeros((y.shape[0], self.num_classes))\n",
    "        for index, y_value in enumerate(y):\n",
    "            class_range_mapping = self.actual_classes_to_class_range[y_value]\n",
    "            y_indicator[index, class_range_mapping] = 1\n",
    "        return y_indicator\n",
    "    \n",
    "    def _get_batches(self, X, y):\n",
    "        for i in range(0, X.shape[0], self.batch_size):\n",
    "            yield (X[i: i + self.batch_size], y[i: i + self.batch_size])\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if self.fit_intercept:\n",
    "            X = np.column_stack((np.ones(len(X)), X))\n",
    "        else:\n",
    "            X = np.column_stack((np.zeros(len(X)), X))\n",
    "        row_length, column_length = X.shape\n",
    "        \n",
    "        # Number of unique classes\n",
    "        self.actual_classes = sorted(np.unique(y))\n",
    "        self.num_classes = len(self.actual_classes)\n",
    "        \n",
    "        # This will generate a list of [0,1,2,3....]. However, we want to map these class labels\n",
    "        # to the original class labels in Y\n",
    "        self.class_range = list(range(self.num_classes))\n",
    "        self.actual_classes_to_class_range = {i: self.actual_classes[index] for index, i in enumerate(self.actual_classes)}\n",
    "        self.class_range_to_actual_classes = {v: k for k, v in self.actual_classes_to_class_range.items()}\n",
    "        \n",
    "        # Convert y to indicator matrix form e.g. If y belongs to class 3, then y = [0,0,1,0..0]\n",
    "        y = self._convert_to_indicator(y)\n",
    "        \n",
    "        # Define the weights, shape = (P + 1, K)\n",
    "        self.weights = np.zeros((column_length, self.num_classes))\n",
    "        \n",
    "        iterations = 0\n",
    "        while(iterations < self.epochs):\n",
    "            iterations += 1\n",
    "            \n",
    "            # Get batches\n",
    "            batches = self._get_batches(X, y)\n",
    "            \n",
    "            # Update weights using Mini batch stochastic gradient descent\n",
    "            for (x_batch, y_batch) in batches:\n",
    "                \n",
    "                # Get raw output\n",
    "                z = x_batch @ self.weights\n",
    "                \n",
    "                # Calculate class probabilities from raw output, shape = (B, K); B = batch size\n",
    "                P = self._softmax(z)\n",
    "                \n",
    "                # Calculate gradient\n",
    "                grad = x_batch.T @ (P - y_batch)\n",
    "                \n",
    "                # Update weights\n",
    "                self.weights -= self.learning_rate * grad\n",
    "                \n",
    "    def predict_proba(self, X):\n",
    "        \n",
    "        if self.fit_intercept:\n",
    "            X = np.column_stack((np.ones(len(X)), X))\n",
    "        else:\n",
    "            X = np.column_stack((np.zeros(len(X)), X))\n",
    "        \n",
    "        z = X @ self.weights\n",
    "        predicted_probs = self._softmax(z)\n",
    "        return predicted_probs\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predicted_probs = self.predict_proba(X)\n",
    "        preds = self._get_true_class_labels(predicted_probs)\n",
    "        return preds\n",
    "    \n",
    "    def get_accuracy(self, y, y_hat):\n",
    "        return sum(y == y_hat)*100/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X_iris, y_iris = iris.data, iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_logreg = MultiClassLogisticRegression(epochs = 100, batch_size = 60, learning_rate = 0.005)\n",
    "multi_logreg.fit(X_iris, y_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97.33333333333333"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_logreg.get_accuracy(preds, y_iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron():\n",
    "    \n",
    "    def __init__(self, w = None, b = None, termination_steps = 0):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        self.termination_steps = termination_steps\n",
    "        self.prev_weights = []\n",
    "        self.prev_bias = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.w = np.zeros((1, X.shape[1]))\n",
    "        self.b = 0\n",
    "        \n",
    "        while(True):\n",
    "            \n",
    "            # Update w, b by iterating over each row of X\n",
    "            misclassified_count = 0\n",
    "            for index in range(len(y)):\n",
    "                \n",
    "                # Update when a data point gets misclassified\n",
    "                pred = np.sign(np.dot(self.w, X[index]) + self.b)[0]\n",
    "                if pred != y[index]:\n",
    "                    misclassified_count += 1\n",
    "                    self.w += X[index]*y[index]\n",
    "                    self.b += y[index]\n",
    "            \n",
    "            # Termination condition\n",
    "            if (misclassified_count == 0) or (misclassified_count >= 0.3*len(y) \\\n",
    "                                              and self.termination_steps >= 1e5):\n",
    "                break\n",
    "                \n",
    "            self.prev_weights.append(self.w.copy())\n",
    "            self.prev_bias.append(copy(self.b))\n",
    "            self.termination_steps += 1\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.sign(self.w @ X.T + self.b)[0]\n",
    "    \n",
    "    def get_accuracy(self, y, y_hat):\n",
    "        return sum(y == y_hat)*100/len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, \n",
    "                 data = None, \n",
    "                 split_variable = None,\n",
    "                 split_variable_value = None,\n",
    "                 left = None, \n",
    "                 right = None, \n",
    "                 depth = 0,\n",
    "                 criterion_value = None):\n",
    "        self.data = data\n",
    "        self.split_variable = split_variable\n",
    "        self.split_variable_value = split_variable_value\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.criterion_value = criterion_value\n",
    "        self.depth = depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier():\n",
    "    def __init__(self,\n",
    "                 root = None, \n",
    "                 criterion = \"gini\", \n",
    "                 max_depth = None,\n",
    "                 significance = None,\n",
    "                 significance_threshold = 3.841,\n",
    "                 min_samples_split = 1):\n",
    "        self.root = root\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.significance = significance\n",
    "        self.significance_threshold = significance_threshold\n",
    "        \n",
    "        self.split_score_funcs = {'gini': self._calculate_gini_values,\n",
    "                                  'entropy': self._calculate_entropy_values}\n",
    "    \n",
    "    def _get_proportions(self, X):\n",
    "        counts_of_classes_of_y = X['Y'].value_counts()\n",
    "        proportions_of_classes_of_y = counts_of_classes_of_y/X.shape[0]\n",
    "        return proportions_of_classes_of_y\n",
    "    \n",
    "    def _get_entropy_index(self, X):\n",
    "        return\n",
    "    \n",
    "    def _calculate_entropy_values(self, X, feature):\n",
    "        return\n",
    "    \n",
    "    def _get_gini_index(self, X):\n",
    "        if X.empty:\n",
    "            return 0\n",
    "        \n",
    "        # Get proportion of all classes of y in X\n",
    "        proportions = self._get_proportions(X)\n",
    "        \n",
    "        # Calculate the gini index\n",
    "        gini_index = 1 - np.sum(proportions**2)\n",
    "        return gini_index\n",
    "    \n",
    "    def _calculate_gini_values(self, X, feature):\n",
    "        \n",
    "        # Calculate unique values of X. For a feature, there are different\n",
    "        # values on which that feature can be split\n",
    "        classes = X[feature].unique()\n",
    "        \n",
    "        # Calculate the gini value for a split on each unique value of the feature.\n",
    "        best_gini_score = 999\n",
    "        best_feature_value = \"\"\n",
    "        for unique_value in classes:\n",
    "            # Split data\n",
    "            left_split = X[X[feature] <= unique_value]\n",
    "            right_split = X[X[feature] > unique_value]\n",
    "            \n",
    "            # Get gini scores of left, right nodes\n",
    "            gini_value_left_split = self._get_gini_index(left_split)\n",
    "            gini_value_right_split = self._get_gini_index(right_split)\n",
    "            \n",
    "            # Combine the 2 scores to get the overall score for the split\n",
    "            gini_score_of_current_value = (left_split.shape[0]/X.shape[0]) * gini_value_left_split + \\\n",
    "                                           (right_split.shape[0]/X.shape[0]) * gini_value_right_split\n",
    "            \n",
    "            if gini_score_of_current_value < best_gini_score:\n",
    "                best_gini_score = gini_score_of_current_value\n",
    "                best_feature_value = unique_value\n",
    "        \n",
    "        return best_gini_score, best_feature_value\n",
    "    \n",
    "    def _get_best_split_feature(self, X):\n",
    "        best_split_score = 999\n",
    "        best_feature = \"\"\n",
    "        best_value = None\n",
    "        columns = X.drop('Y', 1).columns\n",
    "        \n",
    "        for feature in columns:\n",
    "            \n",
    "            # Calculate the best split score and the best value\n",
    "            # for the current feature.\n",
    "            split_score, feature_value = self.split_score_funcs[self.criterion](X, feature)\n",
    "            \n",
    "            # Compare this feature's split score with the current best score\n",
    "            if split_score < best_split_score:\n",
    "                best_split_score = split_score\n",
    "                best_feature = feature\n",
    "                best_value = feature_value\n",
    "        \n",
    "        return best_feature, best_value, best_split_score\n",
    "    \n",
    "    def _split_data(self, X, X_depth = None):\n",
    "        \n",
    "        # Return if dataframe is empty, depth exceeds maximum depth or sample size exceeds\n",
    "        # minimum sample size required to split.\n",
    "        if X.empty or len(X['Y'].value_counts()) == 1 or X_depth == self.max_depth \\\n",
    "                            or X.shape[0] <= self.min_samples_split:\n",
    "            return None, None, \"\", \"\", 0\n",
    "        \n",
    "        # Calculate the best feature to split X\n",
    "        best_feature, best_value, best_score = self._get_best_split_feature(X)\n",
    "        \n",
    "        if best_feature == \"\":\n",
    "            return None, None, \"\", \"\", 0\n",
    "        \n",
    "        # Create left and right nodes\n",
    "        X_left = Node(data = X[X[best_feature] <= best_value].drop(best_feature, 1),\n",
    "                      depth = X_depth + 1)\n",
    "        X_right = Node(data = X[X[best_feature] > best_value].drop(best_feature, 1),\n",
    "                       depth = X_depth + 1)\n",
    "\n",
    "        return X_left, X_right, best_feature, best_value, best_score\n",
    "    \n",
    "    def _fit(self, X):\n",
    "        \n",
    "        # Handle the initial case\n",
    "        if not (type(X) == Node):\n",
    "            X = Node(data = X)\n",
    "            self.root = X\n",
    "        \n",
    "        # Get the splits\n",
    "        X_left, X_right, best_feature, best_value, best_score = self._split_data(X.data, X.depth)\n",
    "        \n",
    "        # Assign attributes of node X\n",
    "        X.left = X_left\n",
    "        X.right = X_right\n",
    "        X.split_variable = best_feature\n",
    "        X.split_variable_value = round(best_value, 3) if type(best_value) != str else best_value\n",
    "        X.criterion_value = round(best_score, 3)\n",
    "        \n",
    "        # Return if no best variable found to split on. \n",
    "        # This means you have reached the leaf node.\n",
    "        if best_feature == \"\":\n",
    "            return\n",
    "        \n",
    "        # Recurse for left and right children\n",
    "        self._fit(X_left)\n",
    "        self._fit(X_right)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Combine the 2 and fit\n",
    "        X = pd.DataFrame(X)\n",
    "        X['Y'] = y\n",
    "        self._fit(X)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = pd.DataFrame(X)\n",
    "        X['Y'] = np.nan\n",
    "        \n",
    "        for index, row in X.iterrows():\n",
    "            curr_node = self.root\n",
    "            while(curr_node.left != None and curr_node.right != None):\n",
    "                split_variable = curr_node.split_variable\n",
    "                split_variable_value = curr_node.split_variable_value\n",
    "                if X.loc[index, split_variable] <= split_variable_value:\n",
    "                    curr_node = curr_node.left\n",
    "                else:\n",
    "                    curr_node = curr_node.right\n",
    "            \n",
    "            # Assign Y value\n",
    "            X.loc[index, 'Y'] = max(curr_node.data['Y'].values, key = list(curr_node.data['Y'].values).count)    \n",
    "        \n",
    "        preds = X['Y'].values\n",
    "        return preds\n",
    "    \n",
    "    def display_tree_structure(self):\n",
    "        tree = Digraph('DecisionTree', \n",
    "                       filename = 'tree.dot', \n",
    "                       node_attr = {'shape': 'box'})\n",
    "        tree.attr(size = '10, 20')\n",
    "        root = self.root\n",
    "        id = 0\n",
    "        \n",
    "        # queue with nodes to process\n",
    "        nodes = [(None, root, 'root')]\n",
    "        while nodes:\n",
    "            parent, node, x = nodes.pop(0)\n",
    "            \n",
    "            # Generate appropriate labels for the nodes\n",
    "            value_counts_length = len(node.data['Y'].value_counts())\n",
    "            if node.split_variable != \"\":\n",
    "                split_variable = node.split_variable\n",
    "                split_variable_value = node.split_variable_value\n",
    "            else:\n",
    "                split_variable = \"None\"\n",
    "                \n",
    "            if value_counts_length > 1:\n",
    "                label = str(split_variable) + '\\n' + self.criterion + \" = \" + str(split_variable_value)\n",
    "            else:\n",
    "                label = \"None\"\n",
    "            \n",
    "            # Make edges between the nodes\n",
    "            tree.node(name = str(id), \n",
    "                      label = label, \n",
    "                      color = 'black', \n",
    "                      fillcolor = 'goldenrod2', \n",
    "                      style = 'filled')\n",
    "            \n",
    "            if parent is not None:\n",
    "                if x == 'left':\n",
    "                    tree.edge(parent, str(id), color = 'sienna', \n",
    "                              style = 'filled', label = '<=' + ' ' + str(split_variable_value))\n",
    "                else:\n",
    "                    tree.edge(parent, str(id), color = 'sienna', \n",
    "                              style = 'filled', label = '>' + ' ' + str(split_variable_value))\n",
    "\n",
    "            if node.left is not None:\n",
    "                nodes.append((str(id), node.left, 'left'))\n",
    "\n",
    "            if node.right is not None:\n",
    "                nodes.append((str(id), node.right, 'right'))\n",
    "            id += 1\n",
    "            \n",
    "        return tree\n",
    "        \n",
    "    def get_accuracy(self, y, y_hat):\n",
    "        return sum(y == y_hat)*100/len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeRegressor():\n",
    "    def __init__(self,\n",
    "                 root = None, \n",
    "                 criterion = \"mse\", \n",
    "                 max_depth = None,\n",
    "                 significance = None,\n",
    "                 significance_threshold = 3.841,\n",
    "                 min_samples_split = 1):\n",
    "        self.root = root\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.significance = significance\n",
    "        self.significance_threshold = significance_threshold\n",
    "        \n",
    "        self.split_score_funcs = {'mse': self._calculate_mse_values}\n",
    "    \n",
    "    def _get_mse(self, X):\n",
    "        if X.empty:\n",
    "            return 0\n",
    "        \n",
    "        # Calculate the mean square error with respect to the mean\n",
    "        y = X['Y']\n",
    "        y_mean = np.mean(y)\n",
    "        mse = np.mean((y - y_mean)**2)\n",
    "        return mse\n",
    "    \n",
    "    def _calculate_mse_values(self, X, feature):\n",
    "        \n",
    "        # Calculate unique values of X. For a feature, there are different\n",
    "        # values on which that feature can be split\n",
    "        classes = X[feature].unique()\n",
    "        \n",
    "        # Calculate the gini value for a split on each unique value of the feature.\n",
    "        best_mse_score = 999\n",
    "        best_feature_value = \"\"\n",
    "        for unique_value in classes:\n",
    "            # Split data\n",
    "            left_split = X[X[feature] <= unique_value]\n",
    "            right_split = X[X[feature] > unique_value]\n",
    "            \n",
    "            # Get gini scores of left, right nodes\n",
    "            mse_value_left_split = self._get_mse(left_split)\n",
    "            mse_value_right_split = self._get_mse(right_split)\n",
    "            \n",
    "            # Combine the 2 scores to get the overall score for the split\n",
    "            mse_score_of_current_value = (left_split.shape[0]/X.shape[0]) * mse_value_left_split + \\\n",
    "                                           (right_split.shape[0]/X.shape[0]) * mse_value_right_split\n",
    "            \n",
    "            if mse_score_of_current_value < best_mse_score:\n",
    "                best_mse_score = mse_score_of_current_value\n",
    "                best_feature_value = unique_value\n",
    "            \n",
    "        return best_mse_score, best_feature_value\n",
    "    \n",
    "    def _get_best_split_feature(self, X):\n",
    "        best_split_score = 999\n",
    "        best_feature = \"\"\n",
    "        best_value = None\n",
    "        columns = X.drop('Y', 1).columns\n",
    "        \n",
    "        for feature in columns:\n",
    "            \n",
    "            # Calculate the best split score and the best value\n",
    "            # for the current feature.\n",
    "            split_score, feature_value = self.split_score_funcs[self.criterion](X, feature)\n",
    "            \n",
    "            # Compare this feature's split score with the current best score\n",
    "            if split_score < best_split_score:\n",
    "                best_split_score = split_score\n",
    "                best_feature = feature\n",
    "                best_value = feature_value\n",
    "        \n",
    "        return best_feature, best_value, best_split_score\n",
    "    \n",
    "    def _split_data(self, X, X_depth = None):\n",
    "        \n",
    "        # Return if dataframe is empty, depth exceeds maximum depth or sample size exceeds\n",
    "        # minimum sample size required to split.\n",
    "        if X.empty or len(X['Y'].value_counts()) == 1 or X_depth == self.max_depth \\\n",
    "                            or X.shape[0] <= self.min_samples_split:\n",
    "            return None, None, \"\", \"\", 0\n",
    "        \n",
    "        # Calculate the best feature to split X\n",
    "        best_feature, best_value, best_score = self._get_best_split_feature(X)\n",
    "        \n",
    "        if best_feature == \"\":\n",
    "            return None, None, \"\", \"\", 0\n",
    "        \n",
    "        # Create left and right nodes\n",
    "        X_left = Node(data = X[X[best_feature] <= best_value].drop(best_feature, 1),\n",
    "                      depth = X_depth + 1)\n",
    "        X_right = Node(data = X[X[best_feature] > best_value].drop(best_feature, 1),\n",
    "                       depth = X_depth + 1)\n",
    "\n",
    "        return X_left, X_right, best_feature, best_value, best_score\n",
    "    \n",
    "    def _fit(self, X):\n",
    "        \n",
    "        # Handle the initial case\n",
    "        if not (type(X) == Node):\n",
    "            X = Node(data = X)\n",
    "            self.root = X\n",
    "        \n",
    "        # Get the splits\n",
    "        X_left, X_right, best_feature, best_value, best_score = self._split_data(X.data, X.depth)\n",
    "        \n",
    "        # Assign attributes of node X\n",
    "        X.left = X_left\n",
    "        X.right = X_right\n",
    "        X.split_variable = best_feature\n",
    "        X.split_variable_value = round(best_value, 3) if type(best_value) != str else best_value\n",
    "        X.criterion_value = round(best_score, 3)\n",
    "        \n",
    "        # Return if no best variable found to split on. \n",
    "        # This means you have reached the leaf node.\n",
    "        if best_feature == \"\":\n",
    "            return\n",
    "        \n",
    "        # Recurse for left and right children\n",
    "        self._fit(X_left)\n",
    "        self._fit(X_right)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        # Combine the 2 and fit\n",
    "        X = pd.DataFrame(X)\n",
    "        X['Y'] = y\n",
    "        self._fit(X)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = pd.DataFrame(X)\n",
    "        X['Y'] = np.nan\n",
    "        \n",
    "        preds = []\n",
    "        for index, row in X.iterrows():\n",
    "            curr_node = self.root\n",
    "            while(curr_node.left != None and curr_node.right != None):\n",
    "                split_variable = curr_node.split_variable\n",
    "                split_variable_value = curr_node.split_variable_value\n",
    "                if X.loc[index, split_variable] <= split_variable_value:\n",
    "                    curr_node = curr_node.left\n",
    "                else:\n",
    "                    curr_node = curr_node.right\n",
    "            \n",
    "            # Get prediction\n",
    "            preds.append(np.mean(curr_node.data['Y'].values))\n",
    "        \n",
    "        preds = X['Y'].values\n",
    "        return preds\n",
    "    \n",
    "    def display_tree_structure(self):\n",
    "        tree = Digraph('DecisionTree', \n",
    "                       filename = 'tree.dot', \n",
    "                       node_attr = {'shape': 'box'})\n",
    "        tree.attr(size = '10, 20')\n",
    "        root = self.root\n",
    "        id = 0\n",
    "        \n",
    "        # queue with nodes to process\n",
    "        nodes = [(None, root, 'root')]\n",
    "        while nodes:\n",
    "            parent, node, x = nodes.pop(0)\n",
    "            \n",
    "            # Generate appropriate labels for the nodes\n",
    "            value_counts_length = len(node.data['Y'].value_counts())\n",
    "            if node.split_variable != \"\":\n",
    "                split_variable = node.split_variable\n",
    "                split_variable_value = node.split_variable_value\n",
    "            else:\n",
    "                split_variable = \"None\"\n",
    "                \n",
    "            if value_counts_length > 1:\n",
    "                label = split_variable + '\\n' + self.criterion + \" = \" + \\\n",
    "                            str(node.criterion_value)\n",
    "            else:\n",
    "                label = \"None\"\n",
    "            \n",
    "            # Make edges between the nodes\n",
    "            tree.node(name = str(id), \n",
    "                      label = label, \n",
    "                      color = 'black', \n",
    "                      fillcolor = 'goldenrod2', \n",
    "                      style = 'filled')\n",
    "            \n",
    "            if parent is not None:\n",
    "                if x == 'left':\n",
    "                    tree.edge(parent, str(id), color = 'sienna', \n",
    "                              style = 'filled', label = '<=' + ' ' + str(split_variable_value))\n",
    "                else:\n",
    "                    tree.edge(parent, str(id), color = 'sienna', \n",
    "                              style = 'filled', label = '>' + ' ' + str(split_variable_value))\n",
    "\n",
    "            if node.left is not None:\n",
    "                nodes.append((str(id), node.left, 'left'))\n",
    "\n",
    "            if node.right is not None:\n",
    "                nodes.append((str(id), node.right, 'right'))\n",
    "            id += 1\n",
    "            \n",
    "        return tree\n",
    "        \n",
    "    def get_error(self, y, y_hat):\n",
    "        return np.mean((y - y_hat)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaboostClassifier():\n",
    "    \n",
    "    def __init__(self, n_estimators = 100, weights = None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.weights = weights\n",
    "        self.alphas = []\n",
    "        \n",
    "    def _convert_y(self, y):\n",
    "        self.actual_classes = sorted(np.unique(y))\n",
    "        self.class_range = [-1, 1]\n",
    "        self.class_range_to_actual_classes = dict(zip(*(self.class_range, self.actual_classes)))\n",
    "        self.actual_classes_to_class_range = dict(zip(*(self.actual_classes, self.class_range)))\n",
    "        \n",
    "        y_ = np.array([self.actual_classes_to_class_range[i] for i in y])\n",
    "        return y_\n",
    "    \n",
    "    def _get_true_class_labels(self, labels):\n",
    "        true_labels = np.array([self.class_range_to_actual_classes[i] for i in labels])\n",
    "        return true_labels\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        # Convert y to {-1, 1}\n",
    "        y = self._convert_y(y)\n",
    "        \n",
    "        # Initialise weights for all data points\n",
    "        row_length = X.shape[0]\n",
    "        self.weights = np.ones((self.n_estimators, row_length))\n",
    "        self.alphas = np.zeros((self.n_estimators, 1))\n",
    "        self.estimators = np.empty((self.n_estimators, 1), dtype = object)\n",
    "        \n",
    "        time_step = 0\n",
    "        for time_step in range(self.n_estimators):\n",
    "            \n",
    "            # Use a weak classifier to fit on data\n",
    "            weak_classifier = LogisticRegression(solver = \"sgd\", epochs = 20)\n",
    "            weak_classifier.fit(X, y)\n",
    "            pred = weak_classifier.predict(X)\n",
    "            \n",
    "            # Get weighted error \n",
    "            weighted_sample_err = (np.sum((pred != y) * self.weights))/np.sum(self.weights)\n",
    "            \n",
    "            # Alpha for current classifer\n",
    "            alpha_t = 1/2*np.log(((1 - weighted_sample_err)/weighted_sample_err) + 1e-16)\n",
    "            self.alphas[time_step] = alpha_t\n",
    "            self.estimators[time_step] = weak_classifier\n",
    "            \n",
    "            # Update weights of next time step for all data points\n",
    "            if time_step == (self.n_estimators - 1):\n",
    "                break\n",
    "            self.weights[time_step + 1, :] = self.weights[time_step, :] * np.exp(-y * alpha_t * pred)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        preds = []\n",
    "        self.estimators = self.estimators.flatten()\n",
    "        self.alphas = self.alphas.flatten()\n",
    "        for index in range(self.n_estimators):\n",
    "            preds.append(self.alphas[index] * self.estimators[index].predict(X))\n",
    "        \n",
    "        preds = np.sum(preds, 0)\n",
    "        preds = np.sign(preds)\n",
    "        true_preds = self._get_true_class_labels(preds)\n",
    "        return true_preds\n",
    "    \n",
    "    def get_accuracy(self, y, y_hat):\n",
    "        return sum(y == y_hat)*100/len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNeighbours():\n",
    "    def __init__(self, k = 5, distance_metric = 'euclid', problem = \"classify\"):\n",
    "        self.k = k\n",
    "        self.distance_metric = distance_metric\n",
    "        self.problem = problem\n",
    "        self.prediction_functions = {'classify': self._top_k_votes,\n",
    "                                     'regress': self._top_k_mean}\n",
    "        self.eval_functions = {'classify': self._get_accuracy,\n",
    "                               'regress': self._get_mse}\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def _euclidien_distance(self, x):\n",
    "        return np.sqrt(np.sum((x - self.X)**2, axis = 1))\n",
    "    \n",
    "    def _top_k_mean(self, top_k):\n",
    "        return np.mean(top_k)\n",
    "    \n",
    "    def _top_k_votes(self, top_k):\n",
    "        return max(top_k, key = list(top_k).count)\n",
    "    \n",
    "    def _get_accuracy(self, pred, y):\n",
    "        return np.sum((pred == y))*100/len(y)\n",
    "    \n",
    "    def _get_mse(self, pred, y):\n",
    "        return np.mean((pred - y)**2)\n",
    "    \n",
    "    def predict(self, X):\n",
    "\n",
    "        preds = list()\n",
    "        for x in X:\n",
    "            distances = self._euclidien_distance(x)\n",
    "            \n",
    "            # Zip the distances and y values together\n",
    "            distances = zip(*(distances, self.y))\n",
    "            \n",
    "            # Sort the distances list by distance values in descending order\n",
    "            distances = sorted(distances, key = lambda x: x[0])\n",
    "            \n",
    "            # Select top k distances\n",
    "            top_k = distances[:(self.k)]\n",
    "            \n",
    "            top_k = np.array(top_k)\n",
    "            top_k = top_k[:, 1]\n",
    "            \n",
    "            # Calculate mean of y values of these top k data items\n",
    "            pred = self.prediction_functions[self.problem](top_k)\n",
    "            preds.append(pred)\n",
    "        \n",
    "        return preds\n",
    "    \n",
    "    def evaluate(self, pred, y):\n",
    "        eval_func = self.eval_functions[self.problem]\n",
    "        return eval_func(pred, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkClassifier():\n",
    "\n",
    "    SigmoidActivation = \"sigmoid\"\n",
    "    ReLUActivation = \"relu\"\n",
    "    LinearActivation = \"linear\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_hidden_layers = 2,\n",
    "                 learning_rate = 0.1,\n",
    "                 num_neurons_each_layer = None,\n",
    "                 num_neurons_last_layer = 1,\n",
    "                 batch_size = 32,\n",
    "                 epochs = 10,\n",
    "                 weights = None):\n",
    "        self.weights = weights\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_neurons_each_layer = num_neurons_each_layer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.num_neurons_last_layer = num_neurons_last_layer\n",
    "\n",
    "        # Sigmoid activation for other layers. Linear activation for last layer\n",
    "        self.activations = [self.SigmoidActivation] * self.num_hidden_layers + [self.SigmoidActivation]\n",
    "        self.activations_functions = {\n",
    "            self.SigmoidActivation: self._sigmoid,\n",
    "            self.ReLUActivation: self._relu,\n",
    "            self.LinearActivation: self._linear\n",
    "        }\n",
    "        self.activations_derivatives = {\n",
    "            self.SigmoidActivation: self._sigmoid_derivative,\n",
    "            self.ReLUActivation: self._relu_derivative,\n",
    "            self.LinearActivation: self._linear_derivative\n",
    "        }\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        def sigfunc(x):\n",
    "            if x < 0:\n",
    "                return 1 - 1 / (1 + math.exp(x))\n",
    "            else:\n",
    "                return 1 / (1 + math.exp(-x))\n",
    "        x_ = np.array([sigfunc(i) for i in x])\n",
    "        return x_\n",
    "\n",
    "    def _relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def _linear(self, x):\n",
    "        return x\n",
    "\n",
    "    def _sigmoid_derivative(self, x):\n",
    "        return self._sigmoid(x) * (1 - self._sigmoid(x))\n",
    "\n",
    "    def _relu_derivative(self, x):\n",
    "        return (np.ones_like(x) * (x > 0))\n",
    "\n",
    "    def _linear_derivative(self, x):\n",
    "        return np.ones_like(x)\n",
    "\n",
    "    def _mse_loss(self, pred, y):\n",
    "        return np.mean((pred - y) ** 2)\n",
    "\n",
    "    def _initialise_weights(self, input_shape):\n",
    "\n",
    "        self.num_neurons_each_layer.append(self.num_neurons_last_layer)\n",
    "        self.total_layers = self.num_hidden_layers + 1\n",
    "        self.layers = range(self.total_layers)\n",
    "\n",
    "        # Initialising a numpy array of\n",
    "        # shape = (number of hidden layers, number of neurons, number of weights per neuron) to store weights\n",
    "        self.weights = []\n",
    "\n",
    "        # Iterate through the layers\n",
    "        for layer in self.layers:\n",
    "            self.weights.append([])\n",
    "\n",
    "            number_of_neurons_in_this_layer = self.num_neurons_each_layer[layer]\n",
    "            if layer == 0:\n",
    "                self.weights[layer] = np.random.normal(loc = 0,\n",
    "                                                       scale = 0.5,\n",
    "                                                       size = (number_of_neurons_in_this_layer, input_shape))\n",
    "            else:\n",
    "                # Adding 1 for the bias neuron\n",
    "                self.weights[layer] = np.random.normal(loc = 0,\n",
    "                                                       scale = 0.5,\n",
    "                                                       size = (number_of_neurons_in_this_layer,\n",
    "                                                     1 + self.num_neurons_each_layer[layer - 1]))\n",
    "\n",
    "        self.weights = np.array(self.weights)\n",
    "        self.old_weights = deepcopy(self.weights)\n",
    "\n",
    "    def _update_weights(self):\n",
    "        avg_batch_weight_derivatives = np.mean(self.batch_weight_derivatives, axis = 0)\n",
    "        self.weights = self.old_weights - self.learning_rate * avg_batch_weight_derivatives\n",
    "        self.old_weights = deepcopy(self.weights)\n",
    "        self.batch_weight_derivatives = []\n",
    "\n",
    "    def _backward(self, x, y, out):\n",
    "\n",
    "        # The derivatives array will have the same shape as weights array. - one derivative for each\n",
    "        # weight\n",
    "        output_derivatives = deepcopy(out)\n",
    "        weight_derivatives = deepcopy(self.weights)\n",
    "\n",
    "        # Compute the output derivatives\n",
    "        layers_reversed = self.layers[::-1]\n",
    "        for curr_layer in layers_reversed:\n",
    "            next_layer = curr_layer + 1\n",
    "\n",
    "            # For the last layer simply use the formula\n",
    "            if curr_layer == self.total_layers - 1:\n",
    "                output_derivatives[curr_layer] = 2*(out[curr_layer] - y)\n",
    "                continue\n",
    "\n",
    "            # Get the activation derivative function for next layer\n",
    "            activation_for_next_layer = self.activations[next_layer]\n",
    "            activation_derivative = self.activations_derivatives[activation_for_next_layer]\n",
    "\n",
    "            # The next layer output derivatives\n",
    "            next_layer_output_derivatives = output_derivatives[next_layer]\n",
    "\n",
    "            # Calculate the activation derivative. Add a 1 for the bias weight\n",
    "            current_layer_output = out[curr_layer].copy()\n",
    "            current_layer_output = np.insert(current_layer_output, obj = 0, values = 1)\n",
    "            next_layer_activation_derivatives = activation_derivative(self.old_weights[next_layer] @ current_layer_output)\n",
    "            next_layer_activation_derivatives = next_layer_activation_derivatives.reshape(-1, 1)\n",
    "\n",
    "            # Remove the bias from the weights.\n",
    "            next_layer_weights_without_bias = self.old_weights[next_layer][:, 1:]\n",
    "\n",
    "            # Multiply each neuron's activation derivative with its weights. This is the Hadmard product\n",
    "            second_term = next_layer_activation_derivatives * next_layer_weights_without_bias\n",
    "\n",
    "            # Sum over all the neurons in the next layer to get the output derivative for each\n",
    "            # neuron in the current layer. This is because each neuron contributes to all the neurons\n",
    "            # in the next layer.\n",
    "            output_derivatives[curr_layer] = next_layer_output_derivatives @ second_term\n",
    "\n",
    "        # Update the weights using the output derivative calculated above\n",
    "        for curr_layer in layers_reversed:\n",
    "\n",
    "            # Get the activation for this layer and its derivative function\n",
    "            activation_for_this_layer = self.activations[curr_layer]\n",
    "            activation_derivative = self.activations_derivatives[activation_for_this_layer]\n",
    "\n",
    "            # If first layer then use the data as the previous layer.\n",
    "            if curr_layer == 0:\n",
    "                previous_layer_output = x\n",
    "            else:\n",
    "                prev_layer = curr_layer - 1\n",
    "                previous_layer_output = out[prev_layer].copy()\n",
    "                previous_layer_output = np.insert(previous_layer_output, obj = 0, values = 1)\n",
    "\n",
    "            # Current layer output derivatives\n",
    "            curr_layer_output_derivatives = output_derivatives[curr_layer].reshape(-1, 1)\n",
    "\n",
    "            # Get current layer's activation derivatives\n",
    "            curr_layer_activation_derivatives = activation_derivative(self.old_weights[curr_layer] @ previous_layer_output)\n",
    "            curr_layer_activation_derivatives = curr_layer_activation_derivatives.reshape(-1, 1)\n",
    "\n",
    "            # For the current layer multiply each neuron's activation derivatives with all previous layer outputs.\n",
    "            curr_layer_weight_derivatives = curr_layer_output_derivatives * \\\n",
    "                                            curr_layer_activation_derivatives * previous_layer_output\n",
    "            weight_derivatives[curr_layer] = curr_layer_weight_derivatives\n",
    "\n",
    "        # Append the current data point's weight derivatives in the batch derivatives array\n",
    "        self.batch_weight_derivatives.append(weight_derivatives)\n",
    "\n",
    "    def _forward(self, x):\n",
    "        out = []\n",
    "        for curr_layer in self.layers:\n",
    "            out.append([])\n",
    "\n",
    "            # Get the activation for this layer and its function\n",
    "            activation_for_this_layer = self.activations[curr_layer]\n",
    "            activation_function = self.activations_functions[activation_for_this_layer]\n",
    "\n",
    "            if curr_layer == 0:\n",
    "                previous_layer_output = x\n",
    "            else:\n",
    "                previous_layer_output = out[curr_layer - 1].copy()\n",
    "                previous_layer_output = np.insert(previous_layer_output, obj = 0, values = 1)\n",
    "\n",
    "            out[curr_layer] = activation_function(self.weights[curr_layer] @ previous_layer_output)\n",
    "\n",
    "        out = np.array(out)\n",
    "        return out\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        # Add a bias column to X\n",
    "        X_new = np.column_stack((np.ones(len(X)), X))\n",
    "\n",
    "        # Initialise the weights of the network\n",
    "        self._initialise_weights(X_new.shape[1])\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "\n",
    "            # Initialise arrays to store all weight derivatives of the batch\n",
    "            self.batch_weight_derivatives = []\n",
    "\n",
    "            # Update weights using mini-batch stochastic gradient descent\n",
    "            for data_index in range(X_new.shape[0]):\n",
    "                out = self._forward(X_new[data_index])\n",
    "                self._backward(X_new[data_index], y[data_index], out)\n",
    "\n",
    "                if (data_index + 1) % self.batch_size == 0:\n",
    "                    self._update_weights()\n",
    "\n",
    "            predictions = self.predict(X)\n",
    "            loss = self._mse_loss(predictions, y)\n",
    "            print(\"Epoch = \", str(epoch + 1), \" - \", \"Loss = \", str(loss))\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        # Add a bias column to X\n",
    "        X_new = np.column_stack((np.ones(len(X)), X))\n",
    "\n",
    "        preds = []\n",
    "        for x in X_new:\n",
    "            last_layer_out = self._forward(x)[-1]\n",
    "            preds.append(np.argmax(last_layer_out))\n",
    "\n",
    "        preds = np.array(preds)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkRegressor():\n",
    "\n",
    "    SigmoidActivation = \"sigmoid\"\n",
    "    ReLUActivation = \"relu\"\n",
    "    LinearActivation = \"linear\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_hidden_layers = 2,\n",
    "                 learning_rate = 0.1,\n",
    "                 num_neurons_each_layer = None,\n",
    "                 num_neurons_last_layer = 1,\n",
    "                 batch_size = 32,\n",
    "                 epochs = 10,\n",
    "                 weights = None):\n",
    "        self.weights = weights\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_neurons_each_layer = num_neurons_each_layer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.num_neurons_last_layer = num_neurons_last_layer\n",
    "\n",
    "        # Sigmoid activation for other layers. Linear activation for last layer\n",
    "        self.activations = [self.ReLUActivation] * self.num_hidden_layers + [self.LinearActivation]\n",
    "        self.activations_functions = {\n",
    "            self.SigmoidActivation: self._sigmoid,\n",
    "            self.ReLUActivation: self._relu,\n",
    "            self.LinearActivation: self._linear\n",
    "        }\n",
    "        self.activations_derivatives = {\n",
    "            self.SigmoidActivation: self._sigmoid_derivative,\n",
    "            self.ReLUActivation: self._relu_derivative,\n",
    "            self.LinearActivation: self._linear_derivative\n",
    "        }\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        def sigfunc(x):\n",
    "            if x < 0:\n",
    "                return 1 - 1 / (1 + math.exp(x))\n",
    "            else:\n",
    "                return 1 / (1 + math.exp(-x))\n",
    "        x_ = np.array([sigfunc(i) for i in x])\n",
    "        return x_\n",
    "\n",
    "    def _relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def _linear(self, x):\n",
    "        return x\n",
    "\n",
    "    def _sigmoid_derivative(self, x):\n",
    "        return self._sigmoid(x) * (1 - self._sigmoid(x))\n",
    "\n",
    "    def _relu_derivative(self, x):\n",
    "        return (np.ones_like(x) * (x > 0))\n",
    "\n",
    "    def _linear_derivative(self, x):\n",
    "        return np.ones_like(x)\n",
    "\n",
    "    def _mse_loss(self, pred, y):\n",
    "        return np.mean((pred - y) ** 2)\n",
    "\n",
    "    def _initialise_weights(self, input_shape):\n",
    "\n",
    "        self.num_neurons_each_layer.append(self.num_neurons_last_layer)\n",
    "        self.total_layers = self.num_hidden_layers + 1\n",
    "        self.layers = range(self.total_layers)\n",
    "\n",
    "        # Initialising a numpy array of\n",
    "        # shape = (number of hidden layers, number of neurons, number of weights per neuron) to store weights\n",
    "        self.weights = []\n",
    "\n",
    "        # Iterate through the layers\n",
    "        for layer in self.layers:\n",
    "            self.weights.append([])\n",
    "            \n",
    "            number_of_neurons_in_this_layer = self.num_neurons_each_layer[layer]\n",
    "            if layer == 0:\n",
    "                fan_in = input_shape\n",
    "                fan_out = number_of_neurons_in_this_layer\n",
    "                previous_layer_shape = fan_in\n",
    "            else:\n",
    "                fan_in = self.num_neurons_each_layer[layer - 1]\n",
    "                fan_out = number_of_neurons_in_this_layer\n",
    "                previous_layer_shape =  1 + fan_in\n",
    "\n",
    "            self.weights[layer] = np.random.uniform(low = -2./(fan_in + fan_out),\n",
    "                                                    high = 2./(fan_in + fan_out),\n",
    "                                                    size = (number_of_neurons_in_this_layer, \n",
    "                                                           previous_layer_shape))\n",
    "\n",
    "        self.weights = np.array(self.weights)\n",
    "        self.old_weights = deepcopy(self.weights)\n",
    "\n",
    "    def _update_weights(self):\n",
    "        avg_batch_weight_derivatives = np.mean(self.batch_weight_derivatives, axis = 0)\n",
    "        self.weights = self.old_weights - self.learning_rate * avg_batch_weight_derivatives\n",
    "        self.old_weights = deepcopy(self.weights)\n",
    "        self.batch_weight_derivatives = []\n",
    "\n",
    "    def _backward(self, x, y, out):\n",
    "\n",
    "        # The derivatives array will have the same shape as weights array. - one derivative for each\n",
    "        # weight\n",
    "        output_derivatives = deepcopy(out)\n",
    "        weight_derivatives = deepcopy(self.weights)\n",
    "\n",
    "        # Compute the output derivatives\n",
    "        layers_reversed = self.layers[::-1]\n",
    "        for curr_layer in layers_reversed:\n",
    "            next_layer = curr_layer + 1\n",
    "\n",
    "            # For the last layer simply use the formula\n",
    "            if curr_layer == self.total_layers - 1:\n",
    "                output_derivatives[curr_layer] = 2*(out[curr_layer] - y)\n",
    "                continue\n",
    "\n",
    "            # Get the activation derivative function for next layer\n",
    "            activation_for_next_layer = self.activations[next_layer]\n",
    "            activation_derivative = self.activations_derivatives[activation_for_next_layer]\n",
    "\n",
    "            # The next layer output derivatives\n",
    "            next_layer_output_derivatives = output_derivatives[next_layer]\n",
    "\n",
    "            # Calculate the activation derivative. Add a 1 for the bias weight\n",
    "            current_layer_output = out[curr_layer].copy()\n",
    "            current_layer_output = np.insert(current_layer_output, obj = 0, values = 1)\n",
    "            next_layer_activation_derivatives = activation_derivative(self.old_weights[next_layer] @ current_layer_output)\n",
    "            next_layer_activation_derivatives = next_layer_activation_derivatives.reshape(-1, 1)\n",
    "\n",
    "            # Remove the bias from the weights.\n",
    "            next_layer_weights_without_bias = self.old_weights[next_layer][:, 1:]\n",
    "\n",
    "            # Multiply each neuron's activation derivative with its weights. This is the Hadmard product\n",
    "            second_term = next_layer_activation_derivatives * next_layer_weights_without_bias\n",
    "\n",
    "            # Sum over all the neurons in the next layer to get the output derivative for each\n",
    "            # neuron in the current layer. This is because each neuron contributes to all the neurons\n",
    "            # in the next layer.\n",
    "            output_derivatives[curr_layer] = next_layer_output_derivatives @ second_term\n",
    "\n",
    "        # Update the weights using the output derivative calculated above\n",
    "        for curr_layer in layers_reversed:\n",
    "\n",
    "            # Get the activation for this layer and its derivative function\n",
    "            activation_for_this_layer = self.activations[curr_layer]\n",
    "            activation_derivative = self.activations_derivatives[activation_for_this_layer]\n",
    "\n",
    "            # If first layer then use the data as the previous layer.\n",
    "            if curr_layer == 0:\n",
    "                previous_layer_output = x\n",
    "            else:\n",
    "                prev_layer = curr_layer - 1\n",
    "                previous_layer_output = out[prev_layer].copy()\n",
    "                previous_layer_output = np.insert(previous_layer_output, obj = 0, values = 1)\n",
    "\n",
    "            # Current layer output derivatives\n",
    "            curr_layer_output_derivatives = output_derivatives[curr_layer].reshape(-1, 1)\n",
    "\n",
    "            # Get current layer's activation derivatives\n",
    "            curr_layer_activation_derivatives = activation_derivative(self.old_weights[curr_layer] @ previous_layer_output)\n",
    "            curr_layer_activation_derivatives = curr_layer_activation_derivatives.reshape(-1, 1)\n",
    "\n",
    "            # For the current layer multiply each neuron's activation derivatives with all previous layer outputs.\n",
    "            curr_layer_weight_derivatives = curr_layer_output_derivatives * \\\n",
    "                                            curr_layer_activation_derivatives * previous_layer_output\n",
    "            weight_derivatives[curr_layer] = curr_layer_weight_derivatives\n",
    "\n",
    "        # Append the current data point's weight derivatives in the batch derivatives array\n",
    "        self.batch_weight_derivatives.append(weight_derivatives)\n",
    "\n",
    "    def _forward(self, x):\n",
    "        out = []\n",
    "        for curr_layer in self.layers:\n",
    "            out.append([])\n",
    "\n",
    "            # Get the activation for this layer and its function\n",
    "            activation_for_this_layer = self.activations[curr_layer]\n",
    "            activation_function = self.activations_functions[activation_for_this_layer]\n",
    "\n",
    "            if curr_layer == 0:\n",
    "                previous_layer_output = x\n",
    "            else:\n",
    "                previous_layer_output = out[curr_layer - 1].copy()\n",
    "                previous_layer_output = np.insert(previous_layer_output, obj = 0, values = 1)\n",
    "\n",
    "            out[curr_layer] = activation_function(self.weights[curr_layer] @ previous_layer_output)\n",
    "\n",
    "        out = np.array(out)\n",
    "        return out\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        # Add a bias column to X\n",
    "        X_new = np.column_stack((np.ones(len(X)), X))\n",
    "\n",
    "        # Initialise the weights of the network\n",
    "        self._initialise_weights(X_new.shape[1])\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "\n",
    "            # Initialise arrays to store all weight derivatives of the batch\n",
    "            self.batch_weight_derivatives = []\n",
    "\n",
    "            # Update weights using mini-batch stochastic gradient descent\n",
    "            for data_index in range(X_new.shape[0]):\n",
    "                out = self._forward(X_new[data_index])\n",
    "                self._backward(X_new[data_index], y[data_index], out)\n",
    "\n",
    "                if (data_index + 1) % self.batch_size == 0:\n",
    "                    self._update_weights()\n",
    "\n",
    "            predictions = self.predict(X)\n",
    "            loss = self._mse_loss(predictions, y)\n",
    "            print(\"Epoch = \", str(epoch + 1), \" - \", \"Loss = \", str(loss))\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        # Add a bias column to X\n",
    "        X_new = np.column_stack((np.ones(len(X)), X))\n",
    "\n",
    "        preds = []\n",
    "        for x in X_new:\n",
    "            pred = self._forward(x)[-1]\n",
    "            preds.append(pred)\n",
    "\n",
    "        preds = np.array(preds).flatten()\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
