{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 909,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import statistics\n",
    "from sklearn.datasets import load_digits, load_iris, load_boston, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from graphviz import Digraph, Source, Graph\n",
    "from multiprocessing import cpu_count, Pool\n",
    "import sklearn\n",
    "from IPython.display import Math\n",
    "from sklearn.tree import export_graphviz\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('ML1/Tab.delimited.Cleaned.dataset.WITH.variable.labels.csv', sep = '\\t', encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6344, 382)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>session_date</th>\n",
       "      <th>last_update_date</th>\n",
       "      <th>session_last_update_date</th>\n",
       "      <th>referrer</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>session_creation_date</th>\n",
       "      <th>expcomments</th>\n",
       "      <th>numparticipants_actual</th>\n",
       "      <th>numparticipants</th>\n",
       "      <th>...</th>\n",
       "      <th>diseaseforder</th>\n",
       "      <th>quoteorder</th>\n",
       "      <th>flagprimorder</th>\n",
       "      <th>sunkcostorder</th>\n",
       "      <th>anchorinorder</th>\n",
       "      <th>allowedforder</th>\n",
       "      <th>gamblerforder</th>\n",
       "      <th>moneypriorder</th>\n",
       "      <th>imaginedorder</th>\n",
       "      <th>iatorder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2400853</td>\n",
       "      <td>8/28/2013</td>\n",
       "      <td>8/28/13 12:15</td>\n",
       "      <td>8/28/13 12:15</td>\n",
       "      <td>abington</td>\n",
       "      <td>8/28/2013</td>\n",
       "      <td>8/28/2013</td>\n",
       "      <td>.</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2400856</td>\n",
       "      <td>8/28/2013</td>\n",
       "      <td>8/28/13 12:13</td>\n",
       "      <td>8/28/13 12:13</td>\n",
       "      <td>abington</td>\n",
       "      <td>8/28/2013</td>\n",
       "      <td>8/28/2013</td>\n",
       "      <td>.</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2400860</td>\n",
       "      <td>8/28/2013</td>\n",
       "      <td>8/28/13 12:15</td>\n",
       "      <td>8/28/13 12:15</td>\n",
       "      <td>abington</td>\n",
       "      <td>8/28/2013</td>\n",
       "      <td>8/28/2013</td>\n",
       "      <td>.</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2400868</td>\n",
       "      <td>8/28/2013</td>\n",
       "      <td>8/28/13 12:12</td>\n",
       "      <td>8/28/13 12:12</td>\n",
       "      <td>abington</td>\n",
       "      <td>8/28/2013</td>\n",
       "      <td>8/28/2013</td>\n",
       "      <td>.</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2400872</td>\n",
       "      <td>8/28/2013</td>\n",
       "      <td>8/28/13 12:11</td>\n",
       "      <td>8/28/13 12:11</td>\n",
       "      <td>abington</td>\n",
       "      <td>8/28/2013</td>\n",
       "      <td>8/28/2013</td>\n",
       "      <td>.</td>\n",
       "      <td></td>\n",
       "      <td>.</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 382 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   session_id session_date last_update_date session_last_update_date  \\\n",
       "0     2400853    8/28/2013    8/28/13 12:15            8/28/13 12:15   \n",
       "1     2400856    8/28/2013    8/28/13 12:13            8/28/13 12:13   \n",
       "2     2400860    8/28/2013    8/28/13 12:15            8/28/13 12:15   \n",
       "3     2400868    8/28/2013    8/28/13 12:12            8/28/13 12:12   \n",
       "4     2400872    8/28/2013    8/28/13 12:11            8/28/13 12:11   \n",
       "\n",
       "   referrer creation_date session_creation_date expcomments  \\\n",
       "0  abington     8/28/2013             8/28/2013           .   \n",
       "1  abington     8/28/2013             8/28/2013           .   \n",
       "2  abington     8/28/2013             8/28/2013           .   \n",
       "3  abington     8/28/2013             8/28/2013           .   \n",
       "4  abington     8/28/2013             8/28/2013           .   \n",
       "\n",
       "  numparticipants_actual numparticipants   ...    diseaseforder quoteorder  \\\n",
       "0                                      5   ...                4          3   \n",
       "1                                      5   ...                7          5   \n",
       "2                                      5   ...                2          8   \n",
       "3                                      5   ...                7          8   \n",
       "4                                      .   ...               10          3   \n",
       "\n",
       "  flagprimorder sunkcostorder anchorinorder allowedforder gamblerforder  \\\n",
       "0             7             2             8             6            10   \n",
       "1             2             6            11             9             1   \n",
       "2             9             3             5            10             6   \n",
       "3            10             6             9             4             1   \n",
       "4            11             6             9             5             4   \n",
       "\n",
       "  moneypriorder imaginedorder iatorder  \n",
       "0             1            11       12  \n",
       "1             4             8       12  \n",
       "2             4            11       12  \n",
       "3            11             2       12  \n",
       "4             1             2       12  \n",
       "\n",
       "[5 rows x 382 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Column Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mixed_columns = ['flagsupplement1', 'flagsupplement2', 'flagsupplement3', 'iatexplicitart1', 'iatexplicitart2',\n",
    "                'iatexplicitart3', 'iatexplicitart4', 'iatexplicitart5', 'iatexplicitart6', 'iatexplicitmath1',\n",
    "                'iatexplicitmath2', 'iatexplicitmath3', 'iatexplicitmath4', 'iatexplicitmath5', 'iatexplicitmath6',\n",
    "                'sysjust1', 'sysjust2', 'sysjust3', 'sysjust4', 'sysjust5', 'sysjust6', 'sysjust7', 'sysjust8',\n",
    "                'mturk.non.US', 'exprace']\n",
    "\n",
    "all_NAN_columns = ['task_status', 'task_sequence', 'beginlocaltime']\n",
    "\n",
    "numeric_columns = ['anchoring3ameter', 'anchoring3bmeter', 'anchoring1akm', 'anchoring1bkm', 'priorexposure8',\n",
    "                    'mturk.duplicate', 'mturk.exclude.null', 'mturk.keep', 'mturk.exclude', 'gamblerfallacya_sd',\n",
    "                    'gamblerfallacyb_sd', 'totexpmissed', 'numparticipants_actual', 'session_id', 'IATfilter',\n",
    "                    'priorexposure3', 'priorexposure4', 'priorexposure5', 'priorexposure6', 'priorexposure7', \n",
    "                    'priorexposure9', 'priorexposure10', 'priorexposure11', 'priorexposure12', 'priorexposure13',\n",
    "                    'numparticipants', 'age', 'anchoring1a', 'priorexposure1', 'priorexposure2', 'mturk.total.mini.exps',\n",
    "                    'anchoring1b', 'anchoring2a', 'anchoring2b', 'anchoring3a', 'anchoring3b', 'anchoring4a',\n",
    "                    'anchoring4b', 'artwarm', 'd_donotuse', 'ethnicity', 'flagdv1', 'flagdv2', 'flagdv3',\n",
    "                    'flagdv4', 'flagdv5', 'flagdv6', 'flagdv7', 'flagdv8', 'gamblerfallacya', 'gamblerfallacyb',\n",
    "                    'imaginedexplicit1', 'imaginedexplicit2', 'imaginedexplicit3', 'imaginedexplicit4',\n",
    "                    'major', 'mathwarm', 'moneyagea', 'moneyageb', 'moneygendera', 'moneygenderb', 'omdimc3rt',\n",
    "                    'omdimc3trt', 'quotea', 'quoteb', 'sunkcosta', 'sunkcostb', 'user_id', 'previous_session_id',\n",
    "                    'order', 'meanlatency', 'meanerror', 'block2_meanerror', 'block3_meanerror', 'block5_meanerror',\n",
    "                    'block6_meanerror', 'lat11', 'lat12', 'lat21', 'lat22', 'sd1', 'sd2', 'd_art1', 'd_art2', 'd_art',\n",
    "                    'sunkDV', 'anchoring1', 'anchoring2', 'anchoring3', 'anchoring4', 'Ranchori', 'RAN001',\n",
    "                    'RAN002', 'RAN003', 'Ranch1', 'Ranch2', 'Ranch3', 'Ranch4', 'gambfalDV', 'scalesreca', 'scalesrecb',\n",
    "                    'reciprocityother', 'quotearec', 'quotebrec', 'quote', 'totalflagestimations', 'totalnoflagtimeestimations',\n",
    "                    'flagdv', 'Sysjust', 'moneyfilter', 'Imagineddv', 'IATexpart', 'IATexpmath', 'IATexp.overall',\n",
    "                    'IATEXPfilter', 'scalesorder', 'reciprocorder', 'diseaseforder', 'quoteorder', 'flagprimorder',\n",
    "                    'sunkcostorder', 'anchorinorder', 'allowedforder', 'gamblerforder', 'moneypriorder', 'imaginedorder',\n",
    "                    'iatorder']\n",
    "\n",
    "date_columns = [col for col in data.columns if '_date' in col]\n",
    "\n",
    "non_string_columns = mixed_columns + all_NAN_columns + numeric_columns + date_columns\n",
    "string_columns = sorted([col for col in data.columns if col not in non_string_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert ['', ' ' and .] to NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data.copy()\n",
    "data2 = data2.replace({'': np.nan, ' ': np.nan, '.': np.nan})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Columns to Respective Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Numeric Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anchoring3ameter\n",
      "anchoring3bmeter\n",
      "anchoring1akm\n",
      "anchoring1bkm\n",
      "priorexposure8\n",
      "mturk.duplicate\n",
      "mturk.exclude.null\n",
      "mturk.keep\n",
      "mturk.exclude\n",
      "gamblerfallacya_sd\n",
      "gamblerfallacyb_sd\n",
      "totexpmissed\n",
      "numparticipants_actual\n",
      "session_id\n",
      "IATfilter\n",
      "priorexposure3\n",
      "priorexposure4\n",
      "priorexposure5\n",
      "priorexposure6\n",
      "priorexposure7\n",
      "priorexposure9\n",
      "priorexposure10\n",
      "priorexposure11\n",
      "priorexposure12\n",
      "priorexposure13\n",
      "numparticipants\n",
      "age\n",
      "anchoring1a\n",
      "priorexposure1\n",
      "priorexposure2\n",
      "mturk.total.mini.exps\n",
      "anchoring1b\n",
      "anchoring2a\n",
      "anchoring2b\n",
      "anchoring3a\n",
      "anchoring3b\n",
      "anchoring4a\n",
      "anchoring4b\n",
      "artwarm\n",
      "d_donotuse\n",
      "ethnicity\n",
      "flagdv1\n",
      "flagdv2\n",
      "flagdv3\n",
      "flagdv4\n",
      "flagdv5\n",
      "flagdv6\n",
      "flagdv7\n",
      "flagdv8\n",
      "gamblerfallacya\n",
      "gamblerfallacyb\n",
      "imaginedexplicit1\n",
      "imaginedexplicit2\n",
      "imaginedexplicit3\n",
      "imaginedexplicit4\n",
      "major\n",
      "mathwarm\n",
      "moneyagea\n",
      "moneyageb\n",
      "moneygendera\n",
      "moneygenderb\n",
      "omdimc3rt\n",
      "omdimc3trt\n",
      "quotea\n",
      "quoteb\n",
      "sunkcosta\n",
      "sunkcostb\n",
      "user_id\n",
      "previous_session_id\n",
      "order\n",
      "meanlatency\n",
      "meanerror\n",
      "block2_meanerror\n",
      "block3_meanerror\n",
      "block5_meanerror\n",
      "block6_meanerror\n",
      "lat11\n",
      "lat12\n",
      "lat21\n",
      "lat22\n",
      "sd1\n",
      "sd2\n",
      "d_art1\n",
      "d_art2\n",
      "d_art\n",
      "sunkDV\n",
      "anchoring1\n",
      "anchoring2\n",
      "anchoring3\n",
      "anchoring4\n",
      "Ranchori\n",
      "RAN001\n",
      "RAN002\n",
      "RAN003\n",
      "Ranch1\n",
      "Ranch2\n",
      "Ranch3\n",
      "Ranch4\n",
      "gambfalDV\n",
      "scalesreca\n",
      "scalesrecb\n",
      "reciprocityother\n",
      "quotearec\n",
      "quotebrec\n",
      "quote\n",
      "totalflagestimations\n",
      "totalnoflagtimeestimations\n",
      "flagdv\n",
      "Sysjust\n",
      "moneyfilter\n",
      "Imagineddv\n",
      "IATexpart\n",
      "IATexpmath\n",
      "IATexp.overall\n",
      "IATEXPfilter\n",
      "scalesorder\n",
      "reciprocorder\n",
      "diseaseforder\n",
      "quoteorder\n",
      "flagprimorder\n",
      "sunkcostorder\n",
      "anchorinorder\n",
      "allowedforder\n",
      "gamblerforder\n",
      "moneypriorder\n",
      "imaginedorder\n",
      "iatorder\n"
     ]
    }
   ],
   "source": [
    "for col in numeric_columns:\n",
    "    print(col)\n",
    "    data2[col] = pd.to_numeric(data2[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Date Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session_date\n",
      "last_update_date\n",
      "session_last_update_date\n",
      "creation_date\n",
      "session_creation_date\n",
      "task_creation_date.0\n",
      "task_creation_date.1\n",
      "task_creation_date.2\n",
      "task_creation_date.3\n",
      "task_creation_date.4\n",
      "task_creation_date.5\n",
      "task_creation_date.6\n",
      "task_creation_date.7\n",
      "task_creation_date.8\n",
      "task_creation_date.9\n",
      "task_creation_date.10\n",
      "task_creation_date.11\n",
      "task_creation_date.12\n",
      "task_creation_date.13\n",
      "task_creation_date.14\n",
      "task_creation_date.15\n",
      "task_creation_date.16\n",
      "task_creation_date.17\n",
      "task_creation_date.18\n",
      "task_creation_date.19\n",
      "task_creation_date.20\n",
      "task_creation_date.21\n",
      "task_creation_date.22\n",
      "task_creation_date.23\n",
      "task_creation_date.24\n",
      "task_creation_date.25\n",
      "task_creation_date.26\n",
      "task_creation_date.27\n",
      "task_creation_date.28\n",
      "task_creation_date.29\n",
      "task_creation_date.30\n",
      "task_creation_date.31\n",
      "task_creation_date.32\n",
      "task_creation_date.33\n",
      "task_creation_date.34\n",
      "task_creation_date.35\n",
      "task_creation_date.36\n",
      "task_creation_date.37\n",
      "task_creation_date.38\n",
      "task_creation_date.39\n",
      "task_creation_date.40\n",
      "task_creation_date.41\n",
      "task_creation_date.42\n",
      "task_creation_date.43\n",
      "task_creation_date.44\n",
      "task_creation_date.45\n"
     ]
    }
   ],
   "source": [
    "for col in date_columns:\n",
    "    print(col)\n",
    "    data2[col] = pd.to_datetime(data2[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. String Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ContactGroup\n",
      "MoneyGroup\n",
      "allowedforbidden\n",
      "allowedforbiddenGroup\n",
      "allowedforbiddena\n",
      "allowedforbiddenb\n",
      "anch1group\n",
      "anch2group\n",
      "anch3group\n",
      "anch4group\n",
      "citizenship\n",
      "citizenship2\n",
      "compensation\n",
      "diseaseframinga\n",
      "diseaseframingb\n",
      "expcomments\n",
      "expgender\n",
      "exprunafter\n",
      "exprunafter2\n",
      "feedback\n",
      "filter_$\n",
      "flagGroup\n",
      "flagfilter\n",
      "flagtimeestimate1\n",
      "flagtimeestimate2\n",
      "flagtimeestimate3\n",
      "flagtimeestimate4\n",
      "gainlossDV\n",
      "gainlossgroup\n",
      "gambfalgroup\n",
      "iat_exclude\n",
      "imagineddescribe\n",
      "imptaskto\n",
      "lab_or_online\n",
      "moneyethnicitya\n",
      "moneyethnicityb\n",
      "mturk.Submitted.PaymentReq\n",
      "nativelang\n",
      "nativelang2\n",
      "noflagtimeestimate1\n",
      "noflagtimeestimate2\n",
      "noflagtimeestimate3\n",
      "noflagtimeestimate4\n",
      "o1\n",
      "o10\n",
      "o11\n",
      "o2\n",
      "o3\n",
      "o4\n",
      "o5\n",
      "o6\n",
      "o7\n",
      "o8\n",
      "o9\n",
      "omdimc3\n",
      "partgender\n",
      "politicalid\n",
      "previous_session_schema\n",
      "quoteGroup\n",
      "race\n",
      "reciprocitygroup\n",
      "reciprocityothera\n",
      "reciprocityotherb\n",
      "reciprocityus\n",
      "reciprocityusa\n",
      "reciprocityusb\n",
      "recruitment\n",
      "referrer\n",
      "religion\n",
      "sample\n",
      "scales\n",
      "scalesa\n",
      "scalesb\n",
      "scalesgroup\n",
      "separatedornot\n",
      "session_created_by\n",
      "session_status\n",
      "sex\n",
      "study_name\n",
      "study_url\n",
      "sunkgroup\n",
      "task_id.0\n",
      "task_id.1\n",
      "task_id.10\n",
      "task_id.11\n",
      "task_id.12\n",
      "task_id.13\n",
      "task_id.14\n",
      "task_id.15\n",
      "task_id.16\n",
      "task_id.17\n",
      "task_id.18\n",
      "task_id.19\n",
      "task_id.2\n",
      "task_id.20\n",
      "task_id.21\n",
      "task_id.22\n",
      "task_id.23\n",
      "task_id.24\n",
      "task_id.25\n",
      "task_id.26\n",
      "task_id.27\n",
      "task_id.28\n",
      "task_id.29\n",
      "task_id.3\n",
      "task_id.30\n",
      "task_id.31\n",
      "task_id.32\n",
      "task_id.33\n",
      "task_id.34\n",
      "task_id.35\n",
      "task_id.36\n",
      "task_id.37\n",
      "task_id.38\n",
      "task_id.39\n",
      "task_id.4\n",
      "task_id.40\n",
      "task_id.41\n",
      "task_id.42\n",
      "task_id.43\n",
      "task_id.44\n",
      "task_id.45\n",
      "task_id.5\n",
      "task_id.6\n",
      "task_id.7\n",
      "task_id.8\n",
      "task_id.9\n",
      "task_url.0\n",
      "task_url.1\n",
      "task_url.10\n",
      "task_url.11\n",
      "task_url.12\n",
      "task_url.13\n",
      "task_url.14\n",
      "task_url.15\n",
      "task_url.16\n",
      "task_url.17\n",
      "task_url.18\n",
      "task_url.19\n",
      "task_url.2\n",
      "task_url.20\n",
      "task_url.21\n",
      "task_url.22\n",
      "task_url.23\n",
      "task_url.24\n",
      "task_url.25\n",
      "task_url.26\n",
      "task_url.27\n",
      "task_url.28\n",
      "task_url.29\n",
      "task_url.3\n",
      "task_url.30\n",
      "task_url.31\n",
      "task_url.32\n",
      "task_url.33\n",
      "task_url.34\n",
      "task_url.35\n",
      "task_url.36\n",
      "task_url.37\n",
      "task_url.38\n",
      "task_url.39\n",
      "task_url.4\n",
      "task_url.40\n",
      "task_url.41\n",
      "task_url.42\n",
      "task_url.43\n",
      "task_url.44\n",
      "task_url.45\n",
      "task_url.5\n",
      "task_url.6\n",
      "task_url.7\n",
      "task_url.8\n",
      "task_url.9\n",
      "text\n",
      "us_or_international\n",
      "user_agent\n"
     ]
    }
   ],
   "source": [
    "for col in string_columns:\n",
    "    print(col)\n",
    "    data2[col] = data2[col].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Synthetic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression():\n",
    "    \n",
    "    def __init__(self, bias = None, weights = None, fit_intercept = True):\n",
    "        self.bias = bias\n",
    "        self.weights = weights\n",
    "        self.fit_intercept = fit_intercept\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if self.fit_intercept:\n",
    "            X = np.column_stack((np.ones(len(X)), X))\n",
    "        else:\n",
    "            X = np.column_stack((np.zeros(len(X)), X))\n",
    "        self.all_weights = np.linalg.inv(np.dot(X.T, X)).dot(X.T).dot(y)\n",
    "        self.weights = self.all_weights[1:]\n",
    "        self.bias = self.all_weights[0]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        self.weights = self.weights.reshape(1, -1)\n",
    "        predictions = self.bias + np.dot(self.weights, X.T)\n",
    "        return predictions[0]\n",
    "    \n",
    "    def get_mse(self, y_true, y_pred):\n",
    "        return np.sum((y_true - y_pred)**2)/y_true.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeRegression():\n",
    "    \n",
    "    def __init__(self, \n",
    "                 bias = None, \n",
    "                 weights = None, \n",
    "                 lambda_param = 10, \n",
    "                 fit_intercept = True):\n",
    "        self.bias = bias\n",
    "        self.weights = weights\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.lambda_param = lambda_param\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if self.fit_intercept:\n",
    "            X = np.column_stack((np.ones(len(X)), X))\n",
    "        else:\n",
    "            X = np.column_stack((np.zeros(len(X)), X))\n",
    "            \n",
    "        self.all_weights = np.linalg.inv(np.dot(X.T, X) + \\\n",
    "                            self.lambda_param * np.identity(X.shape[1])).dot(X.T).dot(y)\n",
    "        self.weights = self.all_weights[1:]\n",
    "        self.bias = self.all_weights[0]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        self.weights = self.weights.reshape(1, -1)\n",
    "        predictions = self.bias + np.dot(self.weights, X.T)\n",
    "        return predictions[0]\n",
    "    \n",
    "    def get_mse(self, y_true, y_pred):\n",
    "        return np.sum((y_true - y_pred)**2)/y_true.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LassoRegression():\n",
    "    \n",
    "    def __init__(self, \n",
    "                 bias = None, \n",
    "                 weights = None, \n",
    "                 lambda_param = 10, \n",
    "                 max_iters = 100, \n",
    "                 fit_intercept = True):\n",
    "        self.bias = 0    \n",
    "        self.lambda_param = lambda_param\n",
    "        self.max_iters = max_iters\n",
    "        self.fit_intercept = fit_intercept\n",
    "    \n",
    "    def _soft_threshold(self, x, lambda_):\n",
    "        if x > 0.0 and lambda_ < abs(x):\n",
    "            return x - lambda_\n",
    "        elif x < 0.0 and lambda_ < abs(x):\n",
    "            return x + lambda_\n",
    "        else:\n",
    "            return 0.0\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        if self.fit_intercept:\n",
    "            X = np.column_stack((np.ones(len(X)), X))\n",
    "        else:\n",
    "            X = np.column_stack((np.zeros(len(X)), X))\n",
    "            \n",
    "        row_length, column_length = X.shape\n",
    "        \n",
    "        # Define the weights\n",
    "        self.weights = np.zeros((1, column_length))[0]\n",
    "        if self.fit_intercept:\n",
    "            self.weights[0] = np.sum(y - \\\n",
    "                                np.dot(X[:, 1:], self.weights[1:]))/(X.shape[0])\n",
    "        \n",
    "        #Looping until max number of iterations\n",
    "        for iteration in range(self.max_iters):\n",
    "            start = 1 if self.fit_intercept else 0\n",
    "            \n",
    "            #Looping through each coordinate\n",
    "            for j in range(start, column_length):\n",
    "                    \n",
    "                tmp_weights = self.weights.copy()\n",
    "                tmp_weights[j] = 0.0\n",
    "                r_j = y - np.dot(X, tmp_weights)\n",
    "                arg1 = np.dot(X[:, j], r_j)\n",
    "                arg2 = self.lambda_param * X.shape[0]\n",
    "\n",
    "                self.weights[j] = self._soft_threshold(arg1, arg2)/(X[:, j]**2).sum()\n",
    "\n",
    "                if self.fit_intercept:\n",
    "                    self.weights[0] = np.sum(y - \\\n",
    "                                        np.dot(X[:, 1:], self.weights[1:]))/(X.shape[0])\n",
    "        \n",
    "        self.bias = self.weights[0]\n",
    "        self.weights = self.weights[1:]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        self.weights = self.weights.reshape(1, -1)\n",
    "        predictions = self.bias + np.dot(self.weights, X.T)\n",
    "        return predictions[0]\n",
    "    \n",
    "    def get_mse(self, y_true, y_pred):\n",
    "        return np.sum((y_true - y_pred)**2)/y_true.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1216,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "    \n",
    "    def __init__(self,  \n",
    "                 weights = None, \n",
    "                 bias = None, \n",
    "                 fit_intercept = True,\n",
    "                 decision_threshold = 0.5,\n",
    "                 max_iterations = 30,\n",
    "                 tolerance = 1e-13):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.tolerance = tolerance\n",
    "        self.decision_threshold = decision_threshold\n",
    "        self.max_iterations = max_iterations\n",
    "        \n",
    "    def _sigmoid(self, x):\n",
    "        def sigfunc(x):\n",
    "            if x < 0:\n",
    "                return 1 - 1 / (1 + math.exp(x))\n",
    "            else:\n",
    "                return 1 / (1 + math.exp(-x))\n",
    "        x_ = np.array([sigfunc(i) for i in x])\n",
    "        return x_\n",
    "    \n",
    "    def _log_likelihood(self, X, y):\n",
    "        P = self._sigmoid(X @ self.weights)\n",
    "        P = P.reshape(-1, 1)\n",
    "        log_P = np.log(P + 1e-16)\n",
    "\n",
    "        P_ = 1 - P\n",
    "        log_P_ = np.log(P_ + 1e-16)\n",
    "        log_likelihood = np.sum(y*log_P + (1 - y)*log_P_)\n",
    "        return log_likelihood\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if self.fit_intercept:\n",
    "            X = np.column_stack((np.ones(len(X)), X))\n",
    "        else:\n",
    "            X = np.column_stack((np.zeros(len(X)), X))\n",
    "        y = y.reshape(-1, 1)\n",
    "        row_length, column_length = X.shape\n",
    "        \n",
    "        # Define the weights\n",
    "        self.weights = np.zeros((column_length, 1))\n",
    "        \n",
    "        log_likelihood = self._log_likelihood(X, y)\n",
    "        iterations = 0\n",
    "        delta = np.inf\n",
    "        while(np.abs(delta) > self.tolerance and iterations < self.max_iterations):\n",
    "            iterations += 1\n",
    "            \n",
    "            # Calculate positive class probabilities: p = sigmoid(W*x + b)\n",
    "            P = self._sigmoid(X @ self.weights)\n",
    "            P = P.reshape(-1, 1)\n",
    "\n",
    "            # First derivative of loss w.r.t weights\n",
    "            first_derivative = X.T @ (y - P)\n",
    "            first_derivative = first_derivative.reshape(-1, 1)\n",
    "\n",
    "            # Hessian of loss w.r.t weights\n",
    "            P_ = 1 - P\n",
    "            W = P * P_\n",
    "            W = W.reshape(1, -1)[0]\n",
    "            W = np.diag(W)\n",
    "            second_derivative = X.T @ W @ X\n",
    "\n",
    "            # Weight update using Newton-Rhapson Method\n",
    "            old_weights = deepcopy(self.weights)\n",
    "            self.weights = old_weights + np.linalg.inv(second_derivative) @ first_derivative\n",
    "            \n",
    "            # Calculate new log likelihood\n",
    "            new_log_likelihood = self._log_likelihood(X, y)\n",
    "            delta = log_likelihood - new_log_likelihood\n",
    "            log_likelihood = new_log_likelihood\n",
    "            \n",
    "        self.bias = self.weights[0]\n",
    "        self.weights = self.weights[1:]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = self._sigmoid(X @ self.weights + self.bias)\n",
    "        predictions = predictions > self.decision_threshold # Threshold should be tuned\n",
    "        predictions = predictions.astype(int)\n",
    "        return predictions\n",
    "    \n",
    "    def get_accuracy(self, y, y_hat):\n",
    "        return sum(y == y_hat)*100/len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1263,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassLogisticRegression():\n",
    "    \n",
    "    def __init__(self,  \n",
    "                 weights = None, \n",
    "                 bias = None, \n",
    "                 fit_intercept = True,\n",
    "                 epochs = 50,\n",
    "                 learning_rate = 0.05,\n",
    "                 batch_size = 20):\n",
    "        self.weights = weights\n",
    "        self.learning_rate = learning_rate\n",
    "        self.bias = bias\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def _softmax(self, z):\n",
    "        e_x = np.exp(z)\n",
    "        out = e_x / (1 + e_x.sum(axis = 1, keepdims = True))\n",
    "        return out\n",
    "    \n",
    "    def _get_class_labels(self, P):\n",
    "        return P.argmax(axis = 1)\n",
    "    \n",
    "    def _calculate_cross_entropy(self, y, log_yhat):\n",
    "        return -np.sum(y * log_yhat, axis = 1)\n",
    "    \n",
    "    def _convert_to_indicator(self, y):\n",
    "        y_indicator = np.zeros((y.shape[0], self.num_classes))\n",
    "        for index, y_value in enumerate(y):\n",
    "            class_range_mapping = self.actual_classes_to_class_range[y_value]\n",
    "            y_indicator[index, class_range_mapping] = 1\n",
    "        return y_indicator\n",
    "    \n",
    "    def _get_batches(self, X, y):\n",
    "        for i in range(0, X.shape[0], self.batch_size):\n",
    "            yield (X[i: i + self.batch_size], y[i: i + self.batch_size])\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if self.fit_intercept:\n",
    "            X = np.column_stack((np.ones(len(X)), X))\n",
    "        else:\n",
    "            X = np.column_stack((np.zeros(len(X)), X))\n",
    "        row_length, column_length = X.shape\n",
    "        \n",
    "        # Number of unique classes\n",
    "        self.actual_classes = sorted(np.unique(y))\n",
    "        self.num_classes = len(self.actual_classes)\n",
    "        \n",
    "        # This will generate a list of [0,1,2,3....]. However, we want to map these class labels\n",
    "        # to the original class labels in Y\n",
    "        self.class_range = list(range(self.num_classes))\n",
    "        self.actual_classes_to_class_range = {i: self.actual_classes[index] for index, i in enumerate(self.actual_classes)}\n",
    "        \n",
    "        # Convert y to indicator matrix form e.g. If y belongs to class 3, then y = [0,0,1,0..0]\n",
    "        y = self._convert_to_indicator(y)\n",
    "        \n",
    "        # Define the weights, shape = (P + 1, K)\n",
    "        self.weights = np.zeros((column_length, self.num_classes))\n",
    "        \n",
    "        iterations = 0\n",
    "        while(iterations < self.epochs):\n",
    "            iterations += 1\n",
    "            \n",
    "            # Get batches\n",
    "            batches = self._get_batches(X, y)\n",
    "            \n",
    "            # Update weights using Mini batch stochastic gradient descent\n",
    "            for (x_batch, y_batch) in batches:\n",
    "                \n",
    "                # Get raw output\n",
    "                z = x_batch @ self.weights\n",
    "                \n",
    "                # Calculate class probabilities from raw output, shape = (B, K); B = batch size\n",
    "                P = self._softmax(z)\n",
    "                \n",
    "                # Calculate gradient\n",
    "                grad = x_batch.T @ (P - y_batch)\n",
    "                \n",
    "                # Update weights\n",
    "                self.weights -= self.learning_rate * grad\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        if self.fit_intercept:\n",
    "            X = np.column_stack((np.ones(len(X)), X))\n",
    "        else:\n",
    "            X = np.column_stack((np.zeros(len(X)), X))\n",
    "        \n",
    "        z = X @ self.weights\n",
    "        predicted_probs = self._softmax(z)\n",
    "        preds = self._get_class_labels(predicted_probs)\n",
    "        return preds\n",
    "    \n",
    "    def get_accuracy(self, y, y_hat):\n",
    "        return sum(y == y_hat)*100/len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron():\n",
    "    \n",
    "    def __init__(self, w = None, b = None, termination_steps = 0):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        self.termination_steps = termination_steps\n",
    "        self.prev_weights = []\n",
    "        self.prev_bias = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.w = np.zeros((1, X.shape[1]))\n",
    "        self.b = 0\n",
    "        \n",
    "        while(True):\n",
    "            \n",
    "            # Update w, b by iterating over each row of X\n",
    "            misclassified_count = 0\n",
    "            for index in range(len(y)):\n",
    "                \n",
    "                # Update when a data point gets misclassified\n",
    "                pred = np.sign(np.dot(self.w, X[index]) + self.b)[0]\n",
    "                if pred != y[index]:\n",
    "                    misclassified_count += 1\n",
    "                    self.w += X[index]*y[index]\n",
    "                    self.b += y[index]\n",
    "            \n",
    "            # Termination condition\n",
    "            if (misclassified_count == 0) or (misclassified_count >= 0.3*len(y) \\\n",
    "                                              and self.termination_steps >= 1e5):\n",
    "                break\n",
    "                \n",
    "            self.prev_weights.append(self.w.copy())\n",
    "            self.prev_bias.append(copy(self.b))\n",
    "            self.termination_steps += 1\n",
    "                \n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.sign(np.dot(self.w, X.T) + self.b)[0]\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return self.w, self.b\n",
    "    \n",
    "    def get_error(self, y, y_hat):\n",
    "        return 1 - sum(y == y_hat)/len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, \n",
    "                 data = None, \n",
    "                 split_variable = None,\n",
    "                 split_variable_value = None,\n",
    "                 left = None, \n",
    "                 right = None, \n",
    "                 depth = 0,\n",
    "                 criterion_value = None):\n",
    "        self.data = data\n",
    "        self.split_variable = split_variable\n",
    "        self.split_variable_value = split_variable_value\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.criterion_value = criterion_value\n",
    "        self.depth = depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier():\n",
    "    def __init__(self,\n",
    "                 root = None, \n",
    "                 criterion = \"gini\", \n",
    "                 max_depth = None,\n",
    "                 significance = None,\n",
    "                 significance_threshold = 3.841,\n",
    "                 min_samples_split = 1):\n",
    "        self.root = root\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.significance = significance\n",
    "        self.significance_threshold = significance_threshold\n",
    "        \n",
    "        self.split_score_funcs = {'gini': self._calculate_gini_values,\n",
    "                                  'entropy': self._calculate_entropy_values}\n",
    "    \n",
    "    def _get_proportions(self, X):\n",
    "        counts_of_classes_of_y = X['Y'].value_counts()\n",
    "        proportions_of_classes_of_y = counts_of_classes_of_y/X.shape[0]\n",
    "        return proportions_of_classes_of_y\n",
    "    \n",
    "    def _get_entropy_index(self, X):\n",
    "        return\n",
    "    \n",
    "    def _calculate_entropy_values(self, X, feature):\n",
    "        return\n",
    "    \n",
    "    def _get_gini_index(self, X):\n",
    "        if X.empty:\n",
    "            return 0\n",
    "        \n",
    "        # Get proportion of all classes of y in X\n",
    "        proportions = self._get_proportions(X)\n",
    "        \n",
    "        # Calculate the gini index\n",
    "        gini_index = 1 - np.sum(proportions**2)\n",
    "        return gini_index\n",
    "    \n",
    "    def _calculate_gini_values(self, X, feature):\n",
    "        \n",
    "        # Calculate unique values of X. For a feature, there are different\n",
    "        # values on which that feature can be split\n",
    "        classes = X[feature].unique()\n",
    "        \n",
    "        # Calculate the gini value for a split on each unique value of the feature.\n",
    "        best_gini_score = 999\n",
    "        best_feature_value = \"\"\n",
    "        for unique_value in classes:\n",
    "            # Split data\n",
    "            left_split = X[X[feature] <= unique_value]\n",
    "            right_split = X[X[feature] > unique_value]\n",
    "            \n",
    "            # Get gini scores of left, right nodes\n",
    "            gini_value_left_split = self._get_gini_index(left_split)\n",
    "            gini_value_right_split = self._get_gini_index(right_split)\n",
    "            \n",
    "            # Combine the 2 scores to get the overall score for the split\n",
    "            gini_score_of_current_value = (left_split.shape[0]/X.shape[0]) * gini_value_left_split + \\\n",
    "                                           (right_split.shape[0]/X.shape[0]) * gini_value_right_split\n",
    "            \n",
    "            if gini_score_of_current_value < best_gini_score:\n",
    "                best_gini_score = gini_score_of_current_value\n",
    "                best_feature_value = unique_value\n",
    "            \n",
    "        return best_gini_score, best_feature_value\n",
    "    \n",
    "    def _get_best_split_feature(self, X):\n",
    "        best_split_score = 999\n",
    "        best_feature = \"\"\n",
    "        best_value = None\n",
    "        columns = X.drop('Y', 1).columns\n",
    "        \n",
    "        for feature in columns:\n",
    "            \n",
    "            # Calculate the best split score and the best value\n",
    "            # for the current feature.\n",
    "            split_score, feature_value = self.split_score_funcs[self.criterion](X, feature)\n",
    "            \n",
    "            # Compare this feature's split score with the current best score\n",
    "            if split_score < best_split_score:\n",
    "                best_split_score = split_score\n",
    "                best_feature = feature\n",
    "                best_value = feature_value\n",
    "        \n",
    "        return best_feature, best_value, best_split_score\n",
    "    \n",
    "    def _split_data(self, X, X_depth = None):\n",
    "        \n",
    "        # Return if dataframe is empty, depth exceeds maximum depth or sample size exceeds\n",
    "        # minimum sample size required to split.\n",
    "        if X.empty or len(X['Y'].value_counts()) == 1 or X_depth == self.max_depth \\\n",
    "                            or X.shape[0] <= self.min_samples_split:\n",
    "            return None, None, \"\", \"\", 0\n",
    "        \n",
    "        # Calculate the best feature to split X\n",
    "        best_feature, best_value, best_score = self._get_best_split_feature(X)\n",
    "        \n",
    "        if best_feature == \"\":\n",
    "            return None, None, \"\", \"\", 0\n",
    "        \n",
    "        # Create left and right nodes\n",
    "        X_left = Node(data = X[X[best_feature] <= best_value].drop(best_feature, 1),\n",
    "                      depth = X_depth + 1)\n",
    "        X_right = Node(data = X[X[best_feature] > best_value].drop(best_feature, 1),\n",
    "                       depth = X_depth + 1)\n",
    "\n",
    "        return X_left, X_right, best_feature, best_value, best_score\n",
    "    \n",
    "    def _fit(self, X):\n",
    "        \n",
    "        # Handle the initial case\n",
    "        if not (type(X) == Node):\n",
    "            X = Node(data = X)\n",
    "            self.root = X\n",
    "        \n",
    "        # Get the splits\n",
    "        X_left, X_right, best_feature, best_value, best_score = self._split_data(X.data, X.depth)\n",
    "        \n",
    "        # Assign attributes of node X\n",
    "        X.left = X_left\n",
    "        X.right = X_right\n",
    "        X.split_variable = best_feature\n",
    "        X.split_variable_value = round(best_value, 3) if type(best_value) != str else best_value\n",
    "        X.criterion_value = round(best_score, 3)\n",
    "        \n",
    "        # Return if no best variable found to split on. \n",
    "        # This means you have reached the leaf node.\n",
    "        if best_feature == \"\":\n",
    "            return\n",
    "        \n",
    "        # Recurse for left and right children\n",
    "        self._fit(X_left)\n",
    "        self._fit(X_right)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        # Combine the 2 and fit\n",
    "        X = pd.DataFrame(X)\n",
    "        X['Y'] = y\n",
    "        self._fit(X)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X['Y'] = np.nan\n",
    "        \n",
    "        for index, row in X.iterrows():\n",
    "            curr_node = self.root\n",
    "            while(curr_node.left != None and curr_node.right != None):\n",
    "                split_variable = curr_node.split_variable\n",
    "                split_variable_value = curr_node.split_variable_value\n",
    "                if X.loc[index, split_variable] <= split_variable_value:\n",
    "                    curr_node = curr_node.left\n",
    "                else:\n",
    "                    curr_node = curr_node.right\n",
    "            \n",
    "            # Assign Y value\n",
    "            X.loc[index, 'Y'] = max(curr_node.data['Y'].values, key = list(curr_node.data['Y'].values).count)    \n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def display_tree_structure(self):\n",
    "        tree = Digraph('DecisionTree', \n",
    "                       filename = 'tree.dot', \n",
    "                       node_attr = {'shape': 'box'})\n",
    "        tree.attr(size = '10, 20')\n",
    "        root = self.root\n",
    "        id = 0\n",
    "        \n",
    "        # queue with nodes to process\n",
    "        nodes = [(None, root, 'root')]\n",
    "        while nodes:\n",
    "            parent, node, x = nodes.pop(0)\n",
    "            \n",
    "            # Generate appropriate labels for the nodes\n",
    "            value_counts_length = len(node.data['Y'].value_counts())\n",
    "            if node.split_variable != \"\":\n",
    "                split_variable = node.split_variable\n",
    "                split_variable_value = node.split_variable_value\n",
    "            else:\n",
    "                split_variable = \"None\"\n",
    "                \n",
    "            if value_counts_length > 1:\n",
    "                label = split_variable + '\\n' + self.criterion + \" = \" + \\\n",
    "                            str(node.criterion_value)\n",
    "            else:\n",
    "                label = \"None\"\n",
    "            \n",
    "            # Make edges between the nodes\n",
    "            tree.node(name = str(id), \n",
    "                      label = label, \n",
    "                      color = 'black', \n",
    "                      fillcolor = 'goldenrod2', \n",
    "                      style = 'filled')\n",
    "            \n",
    "            if parent is not None:\n",
    "                if x == 'left':\n",
    "                    tree.edge(parent, str(id), color = 'sienna', \n",
    "                              style = 'filled', label = '<=' + ' ' + str(split_variable_value))\n",
    "                else:\n",
    "                    tree.edge(parent, str(id), color = 'sienna', \n",
    "                              style = 'filled', label = '>' + ' ' + str(split_variable_value))\n",
    "\n",
    "            if node.left is not None:\n",
    "                nodes.append((str(id), node.left, 'left'))\n",
    "\n",
    "            if node.right is not None:\n",
    "                nodes.append((str(id), node.right, 'right'))\n",
    "            id += 1\n",
    "            \n",
    "        return tree\n",
    "        \n",
    "    def get_error(self, y, y_hat):\n",
    "        return 1 - sum(y == y_hat)/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeRegressor():\n",
    "    def __init__(self,\n",
    "                 root = None, \n",
    "                 criterion = \"mse\", \n",
    "                 max_depth = None,\n",
    "                 significance = None,\n",
    "                 significance_threshold = 3.841,\n",
    "                 min_samples_split = 1):\n",
    "        self.root = root\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.significance = significance\n",
    "        self.significance_threshold = significance_threshold\n",
    "        \n",
    "        self.split_score_funcs = {'mse': self._calculate_mse_values}\n",
    "    \n",
    "    def _get_mse(self, X):\n",
    "        if X.empty:\n",
    "            return 0\n",
    "        \n",
    "        # Calculate the mean square error with respect to the mean\n",
    "        y = X['Y']\n",
    "        y_mean = np.mean(y)\n",
    "        mse = np.mean((y - y_mean)**2)\n",
    "        return mse\n",
    "    \n",
    "    def _calculate_mse_values(self, X, feature):\n",
    "        \n",
    "        # Calculate unique values of X. For a feature, there are different\n",
    "        # values on which that feature can be split\n",
    "        classes = X[feature].unique()\n",
    "        \n",
    "        # Calculate the gini value for a split on each unique value of the feature.\n",
    "        best_mse_score = 999\n",
    "        best_feature_value = \"\"\n",
    "        for unique_value in classes:\n",
    "            # Split data\n",
    "            left_split = X[X[feature] <= unique_value]\n",
    "            right_split = X[X[feature] > unique_value]\n",
    "            \n",
    "            # Get gini scores of left, right nodes\n",
    "            mse_value_left_split = self._get_mse(left_split)\n",
    "            mse_value_right_split = self._get_mse(right_split)\n",
    "            \n",
    "            # Combine the 2 scores to get the overall score for the split\n",
    "            mse_score_of_current_value = (left_split.shape[0]/X.shape[0]) * mse_value_left_split + \\\n",
    "                                           (right_split.shape[0]/X.shape[0]) * mse_value_right_split\n",
    "            \n",
    "            if mse_score_of_current_value < best_mse_score:\n",
    "                best_mse_score = mse_score_of_current_value\n",
    "                best_feature_value = unique_value\n",
    "            \n",
    "        return best_mse_score, best_feature_value\n",
    "    \n",
    "    def _get_best_split_feature(self, X):\n",
    "        best_split_score = 999\n",
    "        best_feature = \"\"\n",
    "        best_value = None\n",
    "        columns = X.drop('Y', 1).columns\n",
    "        \n",
    "        for feature in columns:\n",
    "            \n",
    "            # Calculate the best split score and the best value\n",
    "            # for the current feature.\n",
    "            split_score, feature_value = self.split_score_funcs[self.criterion](X, feature)\n",
    "            \n",
    "            # Compare this feature's split score with the current best score\n",
    "            if split_score < best_split_score:\n",
    "                best_split_score = split_score\n",
    "                best_feature = feature\n",
    "                best_value = feature_value\n",
    "        \n",
    "        return best_feature, best_value, best_split_score\n",
    "    \n",
    "    def _split_data(self, X, X_depth = None):\n",
    "        \n",
    "        # Return if dataframe is empty, depth exceeds maximum depth or sample size exceeds\n",
    "        # minimum sample size required to split.\n",
    "        if X.empty or len(X['Y'].value_counts()) == 1 or X_depth == self.max_depth \\\n",
    "                            or X.shape[0] <= self.min_samples_split:\n",
    "            return None, None, \"\", \"\", 0\n",
    "        \n",
    "        # Calculate the best feature to split X\n",
    "        best_feature, best_value, best_score = self._get_best_split_feature(X)\n",
    "        \n",
    "        if best_feature == \"\":\n",
    "            return None, None, \"\", \"\", 0\n",
    "        \n",
    "        # Create left and right nodes\n",
    "        X_left = Node(data = X[X[best_feature] <= best_value].drop(best_feature, 1),\n",
    "                      depth = X_depth + 1)\n",
    "        X_right = Node(data = X[X[best_feature] > best_value].drop(best_feature, 1),\n",
    "                       depth = X_depth + 1)\n",
    "\n",
    "        return X_left, X_right, best_feature, best_value, best_score\n",
    "    \n",
    "    def _fit(self, X):\n",
    "        \n",
    "        # Handle the initial case\n",
    "        if not (type(X) == Node):\n",
    "            X = Node(data = X)\n",
    "            self.root = X\n",
    "        \n",
    "        # Get the splits\n",
    "        X_left, X_right, best_feature, best_value, best_score = self._split_data(X.data, X.depth)\n",
    "        \n",
    "        # Assign attributes of node X\n",
    "        X.left = X_left\n",
    "        X.right = X_right\n",
    "        X.split_variable = best_feature\n",
    "        X.split_variable_value = round(best_value, 3) if type(best_value) != str else best_value\n",
    "        X.criterion_value = round(best_score, 3)\n",
    "        \n",
    "        # Return if no best variable found to split on. \n",
    "        # This means you have reached the leaf node.\n",
    "        if best_feature == \"\":\n",
    "            return\n",
    "        \n",
    "        # Recurse for left and right children\n",
    "        self._fit(X_left)\n",
    "        self._fit(X_right)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        # Combine the 2 and fit\n",
    "        X = pd.DataFrame(X)\n",
    "        X['Y'] = y\n",
    "        self._fit(X)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X['Y'] = np.nan\n",
    "        \n",
    "        preds = []\n",
    "        for index, row in X.iterrows():\n",
    "            curr_node = self.root\n",
    "            while(curr_node.left != None and curr_node.right != None):\n",
    "                split_variable = curr_node.split_variable\n",
    "                split_variable_value = curr_node.split_variable_value\n",
    "                if X.loc[index, split_variable] <= split_variable_value:\n",
    "                    curr_node = curr_node.left\n",
    "                else:\n",
    "                    curr_node = curr_node.right\n",
    "            \n",
    "            # Get prediction\n",
    "            preds.append(np.mean(curr_node.data['Y'].values))\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def display_tree_structure(self):\n",
    "        tree = Digraph('DecisionTree', \n",
    "                       filename = 'tree.dot', \n",
    "                       node_attr = {'shape': 'box'})\n",
    "        tree.attr(size = '10, 20')\n",
    "        root = self.root\n",
    "        id = 0\n",
    "        \n",
    "        # queue with nodes to process\n",
    "        nodes = [(None, root, 'root')]\n",
    "        while nodes:\n",
    "            parent, node, x = nodes.pop(0)\n",
    "            \n",
    "            # Generate appropriate labels for the nodes\n",
    "            value_counts_length = len(node.data['Y'].value_counts())\n",
    "            if node.split_variable != \"\":\n",
    "                split_variable = node.split_variable\n",
    "                split_variable_value = node.split_variable_value\n",
    "            else:\n",
    "                split_variable = \"None\"\n",
    "                \n",
    "            if value_counts_length > 1:\n",
    "                label = split_variable + '\\n' + self.criterion + \" = \" + \\\n",
    "                            str(node.criterion_value)\n",
    "            else:\n",
    "                label = \"None\"\n",
    "            \n",
    "            # Make edges between the nodes\n",
    "            tree.node(name = str(id), \n",
    "                      label = label, \n",
    "                      color = 'black', \n",
    "                      fillcolor = 'goldenrod2', \n",
    "                      style = 'filled')\n",
    "            \n",
    "            if parent is not None:\n",
    "                if x == 'left':\n",
    "                    tree.edge(parent, str(id), color = 'sienna', \n",
    "                              style = 'filled', label = '<=' + ' ' + str(split_variable_value))\n",
    "                else:\n",
    "                    tree.edge(parent, str(id), color = 'sienna', \n",
    "                              style = 'filled', label = '>' + ' ' + str(split_variable_value))\n",
    "\n",
    "            if node.left is not None:\n",
    "                nodes.append((str(id), node.left, 'left'))\n",
    "\n",
    "            if node.right is not None:\n",
    "                nodes.append((str(id), node.right, 'right'))\n",
    "            id += 1\n",
    "            \n",
    "        return tree\n",
    "        \n",
    "    def get_error(self, y, y_hat):\n",
    "        return np.mean((y - y_hat)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaboostRegressor():\n",
    "    \n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def fit(self):\n",
    "        return\n",
    "    \n",
    "    def predict(self):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaboostClassifier():\n",
    "    \n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def fit(self):\n",
    "        return\n",
    "    \n",
    "    def predict(self):\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNeighbours():\n",
    "    def __init__(self, k = 5, distance_metric = 'euclid', problem = \"classify\"):\n",
    "        self.k = k\n",
    "        self.distance_metric = distance_metric\n",
    "        self.problem = problem\n",
    "        self.prediction_functions = {'classify': self._top_k_votes,\n",
    "                                     'regress': self._top_k_mean}\n",
    "        self.eval_functions = {'classify': self._get_accuracy,\n",
    "                               'regress': self._get_mse}\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def _euclidien_distance(self, x):\n",
    "        return np.sqrt(np.sum((x - self.X)**2, axis = 1))\n",
    "    \n",
    "    def _top_k_mean(self, top_k):\n",
    "        return np.mean(top_k)\n",
    "    \n",
    "    def _top_k_votes(self, top_k):\n",
    "        return max(top_k, key = list(top_k).count)\n",
    "    \n",
    "    def _get_accuracy(self, pred, y):\n",
    "        return np.sum((pred == y))*100/len(y)\n",
    "    \n",
    "    def _get_mse(self, pred, y):\n",
    "        return np.mean((pred - y)**2)\n",
    "    \n",
    "    def predict(self, X):\n",
    "\n",
    "        preds = list()\n",
    "        for x in X:\n",
    "            distances = self._euclidien_distance(x)\n",
    "            \n",
    "            # Zip the distances and y values together\n",
    "            distances = zip(*(distances, self.y))\n",
    "            \n",
    "            # Sort the distances list by distance values in descending order\n",
    "            distances = sorted(distances, key = lambda x: x[0])\n",
    "            \n",
    "            # Select top k distances\n",
    "            top_k = distances[:(self.k)]\n",
    "            \n",
    "            top_k = np.array(top_k)\n",
    "            top_k = top_k[:, 1]\n",
    "            \n",
    "            # Calculate mean of y values of these top k data items\n",
    "            pred = self.prediction_functions[self.problem](top_k)\n",
    "            preds.append(pred)\n",
    "        \n",
    "        return preds\n",
    "    \n",
    "    def evaluate(self, pred, y):\n",
    "        eval_func = self.eval_functions[self.problem]\n",
    "        return eval_func(pred, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "\n",
    "    SigmoidActivation = \"sigmoid\"\n",
    "    ReLUActivation = \"relu\"\n",
    "    LinearActivation = \"linear\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_hidden_layers = 2,\n",
    "                 learning_rate = 0.1,\n",
    "                 num_neurons_each_layer = None,\n",
    "                 batch_size = 32,\n",
    "                 epochs = 10,\n",
    "                 weights = None):\n",
    "        self.weights = weights\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_neurons_each_layer = num_neurons_each_layer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Sigmoid activation for other layers. Linear activation for last layer\n",
    "        self.activations = [self.SigmoidActivation] * self.num_hidden_layers + [self.LinearActivation]\n",
    "        self.activations_functions = {\n",
    "            self.SigmoidActivation: self._sigmoid,\n",
    "            self.ReLUActivation: self._relu,\n",
    "            self.LinearActivation: self._linear\n",
    "        }\n",
    "        self.activations_derivatives = {\n",
    "            self.SigmoidActivation: self._sigmoid_derivative,\n",
    "            self.ReLUActivation: self._relu_derivative,\n",
    "            self.LinearActivation: self._linear_derivative\n",
    "        }\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        def sigfunc(x):\n",
    "            if x < 0:\n",
    "                return 1 - 1 / (1 + math.exp(x))\n",
    "            else:\n",
    "                return 1 / (1 + math.exp(-x))\n",
    "        x_ = np.array([sigfunc(i) for i in x])\n",
    "        return x_\n",
    "\n",
    "    def _relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def _linear(self, x):\n",
    "        return x\n",
    "\n",
    "    def _sigmoid_derivative(self, x):\n",
    "        return self._sigmoid(x) * (1 - self._sigmoid(x))\n",
    "\n",
    "    def _relu_derivative(self, x):\n",
    "        return (np.ones_like(x) * (x > 0))\n",
    "\n",
    "    def _linear_derivative(self, x):\n",
    "        return np.ones_like(x)\n",
    "\n",
    "    def _mse_loss(self, pred, y):\n",
    "        return np.mean((pred - y) ** 2)\n",
    "\n",
    "    def _initialise_weights(self, input_shape):\n",
    "\n",
    "        self.num_neurons_each_layer.append(1)\n",
    "        self.total_layers = self.num_hidden_layers + 1\n",
    "        self.layers = range(self.total_layers)\n",
    "\n",
    "        # Initialising a numpy array of\n",
    "        # shape = (number of hidden layers, number of neurons, number of weights per neuron) to store weights\n",
    "        self.weights = []\n",
    "\n",
    "        # Iterate through the layers\n",
    "        for layer in self.layers:\n",
    "            self.weights.append([])\n",
    "\n",
    "            number_of_neurons_in_this_layer = self.num_neurons_each_layer[layer]\n",
    "            if layer == 0:\n",
    "                self.weights[layer] = np.random.normal(loc = 0,\n",
    "                                                       scale = 0.5,\n",
    "                                                       size = (number_of_neurons_in_this_layer, input_shape))\n",
    "            else:\n",
    "                # Adding 1 for the bias neuron\n",
    "                self.weights[layer] = np.random.normal(loc = 0,\n",
    "                                                       scale = 0.5,\n",
    "                                                       size = (number_of_neurons_in_this_layer,\n",
    "                                                     1 + self.num_neurons_each_layer[layer - 1]))\n",
    "\n",
    "        self.weights = np.array(self.weights)\n",
    "        self.old_weights = deepcopy(self.weights)\n",
    "\n",
    "    def _update_weights(self):\n",
    "        avg_batch_weight_derivatives = np.mean(self.batch_weight_derivatives, axis = 0)\n",
    "        self.weights = self.old_weights - self.learning_rate * avg_batch_weight_derivatives\n",
    "        self.old_weights = deepcopy(self.weights)\n",
    "        self.batch_weight_derivatives = []\n",
    "\n",
    "    def _backward(self, x, y, out):\n",
    "\n",
    "        # The derivatives array will have the same shape as weights array. - one derivative for each\n",
    "        # weight\n",
    "        output_derivatives = deepcopy(out)\n",
    "        weight_derivatives = deepcopy(self.weights)\n",
    "\n",
    "        # Compute the output derivatives\n",
    "        layers_reversed = self.layers[::-1]\n",
    "        for curr_layer in layers_reversed:\n",
    "            next_layer = curr_layer + 1\n",
    "\n",
    "            # For the last layer simply use the formula\n",
    "            if curr_layer == self.total_layers - 1:\n",
    "                output_derivatives[curr_layer] = 2*(out[curr_layer] - y)\n",
    "                continue\n",
    "\n",
    "            # Get the activation derivative function for next layer\n",
    "            activation_for_next_layer = self.activations[next_layer]\n",
    "            activation_derivative = self.activations_derivatives[activation_for_next_layer]\n",
    "\n",
    "            # The next layer output derivatives\n",
    "            next_layer_output_derivatives = output_derivatives[next_layer]\n",
    "\n",
    "            # Calculate the activation derivative. Add a 1 for the bias weight\n",
    "            current_layer_output = out[curr_layer].copy()\n",
    "            current_layer_output = np.insert(current_layer_output, obj = 0, values = 1)\n",
    "            next_layer_activation_derivatives = activation_derivative(self.old_weights[next_layer] @ current_layer_output)\n",
    "            next_layer_activation_derivatives = next_layer_activation_derivatives.reshape(-1, 1)\n",
    "\n",
    "            # Remove the bias from the weights.\n",
    "            next_layer_weights_without_bias = self.old_weights[next_layer][:, 1:]\n",
    "\n",
    "            # Multiply each neuron's activation derivative with its weights. This is the Hadmard product\n",
    "            second_term = next_layer_activation_derivatives * next_layer_weights_without_bias\n",
    "\n",
    "            # Sum over all the neurons in the next layer to get the output derivative for each\n",
    "            # neuron in the current layer. This is because each neuron contributes to all the neurons\n",
    "            # in the next layer.\n",
    "            output_derivatives[curr_layer] = next_layer_output_derivatives @ second_term\n",
    "\n",
    "        # Update the weights using the output derivative calculated above\n",
    "        for curr_layer in layers_reversed:\n",
    "\n",
    "            # Get the activation for this layer and its derivative function\n",
    "            activation_for_this_layer = self.activations[curr_layer]\n",
    "            activation_derivative = self.activations_derivatives[activation_for_this_layer]\n",
    "\n",
    "            # If first layer then use the data as the previous layer.\n",
    "            if curr_layer == 0:\n",
    "                previous_layer_output = x\n",
    "            else:\n",
    "                prev_layer = curr_layer - 1\n",
    "                previous_layer_output = out[prev_layer].copy()\n",
    "                previous_layer_output = np.insert(previous_layer_output, obj = 0, values = 1)\n",
    "\n",
    "            # Current layer output derivatives\n",
    "            curr_layer_output_derivatives = output_derivatives[curr_layer].reshape(-1, 1)\n",
    "\n",
    "            # Get current layer's activation derivatives\n",
    "            curr_layer_activation_derivatives = activation_derivative(self.old_weights[curr_layer] @ previous_layer_output)\n",
    "            curr_layer_activation_derivatives = curr_layer_activation_derivatives.reshape(-1, 1)\n",
    "\n",
    "            # For the current layer multiply each neuron's activation derivatives with all previous layer outputs.\n",
    "            curr_layer_weight_derivatives = curr_layer_output_derivatives * \\\n",
    "                                            curr_layer_activation_derivatives * previous_layer_output\n",
    "            weight_derivatives[curr_layer] = curr_layer_weight_derivatives\n",
    "\n",
    "        # Append the current data point's weight derivatives in the batch derivatives array\n",
    "        self.batch_weight_derivatives.append(weight_derivatives)\n",
    "\n",
    "    def _forward(self, x):\n",
    "        out = []\n",
    "        for curr_layer in self.layers:\n",
    "            out.append([])\n",
    "\n",
    "            # Get the activation for this layer and its function\n",
    "            activation_for_this_layer = self.activations[curr_layer]\n",
    "            activation_function = self.activations_functions[activation_for_this_layer]\n",
    "\n",
    "            if curr_layer == 0:\n",
    "                previous_layer_output = x\n",
    "            else:\n",
    "                previous_layer_output = out[curr_layer - 1].copy()\n",
    "                previous_layer_output = np.insert(previous_layer_output, obj = 0, values = 1)\n",
    "\n",
    "            out[curr_layer] = activation_function(self.weights[curr_layer] @ previous_layer_output)\n",
    "\n",
    "        out = np.array(out)\n",
    "        return out\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        # Add a bias column to X\n",
    "        X_new = np.column_stack((np.ones(len(X)), X))\n",
    "\n",
    "        # Initialise the weights of the network\n",
    "        self._initialise_weights(X_new.shape[1])\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "\n",
    "            # Initialise arrays to store all weight derivatives of the batch\n",
    "            self.batch_weight_derivatives = []\n",
    "\n",
    "            # Update weights using mini-batch stochastic gradient descent\n",
    "            for data_index in range(X_new.shape[0]):\n",
    "                out = self._forward(X_new[data_index])\n",
    "                self._backward(X_new[data_index], y[data_index], out)\n",
    "\n",
    "                if (data_index + 1) % self.batch_size == 0:\n",
    "                    self._update_weights()\n",
    "\n",
    "            predictions = self.predict(X)\n",
    "            loss = self._mse_loss(predictions, y)\n",
    "            print(\"Epoch = \", str(epoch + 1), \" - \", \"Loss = \", str(loss))\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        # Add a bias column to X\n",
    "        X_new = np.column_stack((np.ones(len(X)), X))\n",
    "\n",
    "        preds = []\n",
    "        for x in X_new:\n",
    "            pred = self._forward(x)[-1][-1]\n",
    "            preds.append(pred)\n",
    "\n",
    "        preds = np.array(preds)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
